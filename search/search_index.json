{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"yeastdnnexplorer","text":""},{"location":"#introduction","title":"Introduction","text":"<p><code>yeastdnnexplorer</code> is intended to serve as a development environment for exploring different DNN models to infer the relationship between transcription factors and target genes using binding and perturbation expression data.</p>"},{"location":"#installation","title":"Installation","text":"<p>This repo has not yet been added to PyPI. See the developer installation below.</p>"},{"location":"#development","title":"Development","text":"<ol> <li>git clone the repo</li> <li><code>cd</code> into the local version of the repo</li> <li>choose one (or more) of the following (only poetry currently supported)</li> </ol>"},{"location":"#poetry","title":"poetry","text":"<p>You can also install the dependencies using poetry. I prefer setting the following:</p> <pre><code>poetry config virtualenvs.in-project true\n</code></pre> <p>So that the virtual environments are installed in the project directory as <code>.venv</code></p> <p>After cloning and <code>cd</code>ing into the repo, you can install the dependencies with:</p> <pre><code>poetry install\n</code></pre>"},{"location":"#mkdocs","title":"mkdocs","text":"<p>The documentation is build with mkdocs:</p>"},{"location":"#commands","title":"Commands","text":"<p>After building the environment with poetry, you can use <code>poetry run</code> or a poetry shell to execute the following:</p> <ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"data_loaders/real_data_loader/","title":"Real Data Loader","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>A class to load in data from the CSV data for various binding and perturbation experiments.</p> <p>After loading in the data, the data loader will parse the data into the form expected by our models. It will also split the data into training, testing, and validation sets for the model to use.</p> <p>NOTE: Right now the only binding dataset this works with is the brent_nf_cc dataset because it has the same set of genes in each CSV file. This is the case for all of the perturbation datasets, but not for the other 2 binding datasets. In the future we would like to write a dataModule that handles the other 2 binding datasets. For now, you can only pass in a parameter for the title of the perturb response dataset that you want to use, and brent_nf_cc is hardcoded as the binding dataset.</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>class RealDataLoader(LightningDataModule):\n    \"\"\"\n    A class to load in data from the CSV data for various binding and perturbation\n    experiments.\n\n    After loading in the data, the data loader will parse the data into the form\n    expected by our models. It will also split the data into training, testing, and\n    validation sets for the model to use.\n\n    NOTE: Right now the only binding dataset this works with is the brent_nf_cc dataset\n    because it has the same set of genes in each CSV file. This is the case for all of\n    the perturbation datasets, but not for the other 2 binding datasets. In the future\n    we would like to write a dataModule that handles the other 2 binding datasets. For\n    now, you can only pass in a parameter for the title of the perturb response\n    dataset that you want to use, and brent_nf_cc is hardcoded as the binding dataset.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 32,\n        val_size: float = 0.1,\n        test_size: float = 0.1,\n        random_state: int = 42,\n        data_dir_path: str | None = None,\n        perturbation_dataset_title: str = \"hu_reimann_tfko\",\n    ) -&gt; None:\n        \"\"\"\n        Constructor of RealDataLoader.\n\n        :param batch_size: The number of samples in each mini-batch\n        :type batch_size: int\n        :param val_size: The proportion of the dataset to include in the validation\n            split\n        :type val_size: float\n        :param test_size: The proportion of the dataset to include in the test split\n        :type test_size: float\n        :param random_state: The random seed to use for splitting the data (keep this\n            consistent to ensure reproduceability)\n        :type random_state: int\n        :param data_dir_path: The path to the directory containing the CSV files for the\n            binding and perturbation data\n        :type data_dir_path: str\n        :param perturbation_dataset_title: The title of the perturbation dataset to use\n            (one of 'hu_reimann_tfko', 'kemmeren_tfko', or 'mcisaac_oe')\n        :type perturbation_dataset_title: str\n        :raises TypeError: If batch_size is not an positive integer\n        :raises TypeError: If val_size is not a float between 0 and 1 (inclusive)\n        :raises TypeError: If test_size is not a float between 0 and 1 (inclusive)\n        :raises TypeError: If random_state is not an integer\n        :raises ValueError: If val_size + test_size is greater than 1 (i.e. the splits\n            are too large)\n        :raises ValueError: if no data_dir is provided\n        :raises AssertinoError: if the dataset sizes do not match up after reading in\n            the data from the CSV files\n\n        \"\"\"\n        if not isinstance(batch_size, int) or batch_size &lt; 1:\n            raise TypeError(\"batch_size must be a positive integer\")\n        if not isinstance(val_size, (int, float)) or val_size &lt;= 0 or val_size &gt;= 1:\n            raise TypeError(\"val_size must be a float between 0 and 1 (inclusive)\")\n        if not isinstance(test_size, (int, float)) or test_size &lt;= 0 or test_size &gt;= 1:\n            raise TypeError(\"test_size must be a float between 0 and 1 (inclusive)\")\n        if not isinstance(random_state, int):\n            raise TypeError(\"random_state must be an integer\")\n        if data_dir_path is None:\n            raise ValueError(\"data_dir_path must be provided\")\n        if test_size + val_size &gt; 1:\n            raise ValueError(\"val_size + test_size must be less than or equal to 1\")\n        if not isinstance(\n            perturbation_dataset_title, str\n        ) and perturbation_dataset_title in [\n            \"hu_reimann_tfko\",\n            \"kemmeren_tfko\",\n            \"mcisaac_oe\",\n        ]:\n            raise TypeError(\n                \"perturbation_dataset_title must be a string and must be one\"\n                \" of 'hu_reimann_tfko', 'kemmeren_tfko', or 'mcisaac_oe'\"\n            )\n\n        super().__init__()\n        self.batch_size = batch_size\n        self.val_size = val_size\n        self.test_size = test_size\n        self.random_state = random_state\n        self.data_dir_path = data_dir_path\n        self.perturbation_dataset_title = perturbation_dataset_title\n\n        self.final_data_tensor: torch.Tensor = None\n        self.binding_effect_matrix: torch.Tensor | None = None\n        self.perturbation_effect_matrix: torch.Tensor | None = None\n        self.val_dataset: TensorDataset | None = None\n        self.test_dataset: TensorDataset | None = None\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"\n        This function reads in the binding data and perturbation data from the CSV files\n        that we have for these datasets.\n\n        It throws out any genes that are not present in both the binding and\n        perturbation sets, and then structures the data in a way that the model expects\n        and can use\n\n        \"\"\"\n\n        brent_cc_path = os.path.join(self.data_dir_path, \"binding/brent_nf_cc\")\n        brent_nf_csv_files = [\n            f for f in os.listdir(brent_cc_path) if f.endswith(\".csv\")\n        ]\n        perturb_dataset_path = os.path.join(\n            self.data_dir_path, f\"perturbation/{self.perturbation_dataset_title}\"\n        )\n        perturb_dataset_csv_files = [\n            f for f in os.listdir(perturb_dataset_path) if f.endswith(\".csv\")\n        ]\n\n        # get a list of the genes in the binding data csvs\n        # for brent_cc (and the 3 perturb response datasets) the genes are\n        # in the same order in each csv, so it suffices to grab the target_locus_tag\n        # column from the first one\n        brent_cc_genes_ids = pd.read_csv(\n            os.path.join(brent_cc_path, brent_nf_csv_files[0])\n        )[\"target_locus_tag\"]\n        perturb_dataset_genes_ids = pd.read_csv(\n            os.path.join(perturb_dataset_path, perturb_dataset_csv_files[0])\n        )[\"target_locus_tag\"]\n\n        # Get the intersection of the genes in the binding and perturbation data\n        common_genes = set(brent_cc_genes_ids).intersection(perturb_dataset_genes_ids)\n\n        # Read in binding data from csv files\n        binding_data_effects = pd.DataFrame()\n        binding_data_pvalues = pd.DataFrame()\n        for i, file in enumerate(brent_nf_csv_files):\n            file_path = os.path.join(brent_cc_path, file)\n            df = pd.read_csv(file_path)\n\n            # only keep the genes that are in the intersection\n            # of the genes in the binding and perturbation data\n            df = df[df[\"target_locus_tag\"].isin(common_genes)]\n\n            # we need to handle duplicates now\n            # (some datasets have multiple occurrences of the same gene)\n            # we will keep the occurrence with the highest value in the 'effect' column\n            # we can do this by sorting the dataframe by the 'effect' column\n            # in descending order and keeping the fist occurrence of each gene\n            # this does require us to do some additional work later (see how we\n            # are consistently setting the index to 'target_locus_tag',\n            # this ensures all of our datasets are in the same order)\n            df = df.sort_values(\"effect\", ascending=False).drop_duplicates(\n                subset=\"target_locus_tag\", keep=\"first\"\n            )\n\n            # on the first iteration, add target_locus_tag column to the binding data\n            if i == 0:\n                binding_data_effects[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n                binding_data_pvalues[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n                binding_data_effects.set_index(\"target_locus_tag\", inplace=True)\n                binding_data_pvalues.set_index(\"target_locus_tag\", inplace=True)\n\n            binding_data_effects[file] = df.set_index(\"target_locus_tag\")[\"effect\"]\n            binding_data_pvalues[file] = df.set_index(\"target_locus_tag\")[\"pvalue\"]\n\n        # Read in perturbation data from csv files\n        perturbation_effects = pd.DataFrame()\n        perturbation_pvalues = pd.DataFrame()\n        for i, file in enumerate(perturb_dataset_csv_files):\n            file_path = os.path.join(perturb_dataset_path, file)\n            df = pd.read_csv(file_path)\n\n            # only keep the genes that are in the\n            # intersection of the genes in the binding and perturbation data\n            df = df[df[\"target_locus_tag\"].isin(common_genes)]\n\n            # handle duplicates\n            df = df.sort_values(\"effect\", ascending=False).drop_duplicates(\n                subset=\"target_locus_tag\", keep=\"first\"\n            )\n\n            # on the first iteration, add the target_locus_tag\n            # column to the perturbation data\n            if i == 0:\n                perturbation_effects[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n                perturbation_pvalues[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n                perturbation_effects.set_index(\"target_locus_tag\", inplace=True)\n                perturbation_pvalues.set_index(\"target_locus_tag\", inplace=True)\n\n            perturbation_effects[file] = df.set_index(\"target_locus_tag\")[\"effect\"]\n            perturbation_pvalues[file] = df.set_index(\"target_locus_tag\")[\"pvalue\"]\n\n        # shapes should be equal at this point\n        assert binding_data_effects.shape == perturbation_effects.shape\n        assert binding_data_pvalues.shape == perturbation_pvalues.shape\n\n        # reindex so that the rows in binding and perturb data match up\n        # (we need genes to be in the same order)\n        perturbation_effects = perturbation_effects.reindex(binding_data_effects.index)\n        perturbation_pvalues = perturbation_pvalues.reindex(binding_data_pvalues.index)\n\n        # concat the data into the shape expected by the model\n        # we need to first convert the data to tensors\n        binding_data_effects_tensor = torch.tensor(\n            binding_data_effects.values, dtype=torch.float64\n        )\n        binding_data_pvalues_tensor = torch.tensor(\n            binding_data_pvalues.values, dtype=torch.float64\n        )\n        perturbation_effects_tensor = torch.tensor(\n            perturbation_effects.values, dtype=torch.float64\n        )\n        perturbation_pvalues_tensor = torch.tensor(\n            perturbation_pvalues.values, dtype=torch.float64\n        )\n\n        # note that we no longer have a signal / noise tensor\n        # (like for the synthetic data)\n        self.final_data_tensor = torch.stack(\n            [\n                binding_data_effects_tensor,\n                binding_data_pvalues_tensor,\n                perturbation_effects_tensor,\n                perturbation_pvalues_tensor,\n            ],\n            dim=-1,\n        )\n\n    def setup(self, stage: str | None = None) -&gt; None:\n        \"\"\"\n        This function runs after prepare_data finishes and is used to split the data\n        into train, validation, and test sets It ensures that these datasets are of the\n        correct dimensionality and size to be used by the model.\n\n        :param stage: The stage of the data setup (either 'fit' for training, 'validate'\n            for validation, or 'test' for testing), unused for now as the model is not\n            complicated enough to necessitate this\n        :type stage: Optional[str]\n\n        \"\"\"\n        self.binding_effect_matrix = self.final_data_tensor[:, :, 0]\n        self.perturbation_effect_matrix = self.final_data_tensor[:, :, 2]\n\n        # split into train, val, and test\n        X_train, X_temp, Y_train, Y_temp = train_test_split(\n            self.binding_effect_matrix,\n            self.perturbation_effect_matrix,\n            test_size=(self.val_size + self.test_size),\n            random_state=self.random_state,\n        )\n\n        # normalize test_size so that it is a percentage of the remaining data\n        self.test_size = self.test_size / (self.val_size + self.test_size)\n        X_val, X_test, Y_val, Y_test = train_test_split(\n            X_temp, Y_temp, test_size=self.test_size, random_state=self.random_state\n        )\n\n        # Convert to tensors\n        X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n            Y_train, dtype=torch.float32\n        )\n        X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n            Y_val, dtype=torch.float32\n        )\n        X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n            Y_test, dtype=torch.float32\n        )\n\n        # Set our datasets\n        self.train_dataset = TensorDataset(X_train, Y_train)\n        self.val_dataset = TensorDataset(X_val, Y_val)\n        self.test_dataset = TensorDataset(X_test, Y_test)\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the training dataloader, we shuffle to avoid learning based\n        on the order of the data.\n\n        :return: The training dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=True,\n            persistent_workers=True,\n        )\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the validation dataloader.\n\n        :return: The validation dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=False,\n            persistent_workers=True,\n        )\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the testing dataloader.\n\n        :return: The testing dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=False,\n            persistent_workers=True,\n        )\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.__init__","title":"<code>__init__(batch_size=32, val_size=0.1, test_size=0.1, random_state=42, data_dir_path=None, perturbation_dataset_title='hu_reimann_tfko')</code>","text":"<p>Constructor of RealDataLoader.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The number of samples in each mini-batch</p> <code>32</code> <code>val_size</code> <code>float</code> <p>The proportion of the dataset to include in the validation split</p> <code>0.1</code> <code>test_size</code> <code>float</code> <p>The proportion of the dataset to include in the test split</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>The random seed to use for splitting the data (keep this consistent to ensure reproduceability)</p> <code>42</code> <code>data_dir_path</code> <code>str | None</code> <p>The path to the directory containing the CSV files for the binding and perturbation data</p> <code>None</code> <code>perturbation_dataset_title</code> <code>str</code> <p>The title of the perturbation dataset to use (one of \u2018hu_reimann_tfko\u2019, \u2018kemmeren_tfko\u2019, or \u2018mcisaac_oe\u2019)</p> <code>'hu_reimann_tfko'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If batch_size is not an positive integer</p> <code>TypeError</code> <p>If val_size is not a float between 0 and 1 (inclusive)</p> <code>TypeError</code> <p>If test_size is not a float between 0 and 1 (inclusive)</p> <code>TypeError</code> <p>If random_state is not an integer</p> <code>ValueError</code> <p>If val_size + test_size is greater than 1 (i.e. the splits are too large)</p> <code>ValueError</code> <p>if no data_dir is provided</p> <code>AssertinoError</code> <p>if the dataset sizes do not match up after reading in the data from the CSV files</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 32,\n    val_size: float = 0.1,\n    test_size: float = 0.1,\n    random_state: int = 42,\n    data_dir_path: str | None = None,\n    perturbation_dataset_title: str = \"hu_reimann_tfko\",\n) -&gt; None:\n    \"\"\"\n    Constructor of RealDataLoader.\n\n    :param batch_size: The number of samples in each mini-batch\n    :type batch_size: int\n    :param val_size: The proportion of the dataset to include in the validation\n        split\n    :type val_size: float\n    :param test_size: The proportion of the dataset to include in the test split\n    :type test_size: float\n    :param random_state: The random seed to use for splitting the data (keep this\n        consistent to ensure reproduceability)\n    :type random_state: int\n    :param data_dir_path: The path to the directory containing the CSV files for the\n        binding and perturbation data\n    :type data_dir_path: str\n    :param perturbation_dataset_title: The title of the perturbation dataset to use\n        (one of 'hu_reimann_tfko', 'kemmeren_tfko', or 'mcisaac_oe')\n    :type perturbation_dataset_title: str\n    :raises TypeError: If batch_size is not an positive integer\n    :raises TypeError: If val_size is not a float between 0 and 1 (inclusive)\n    :raises TypeError: If test_size is not a float between 0 and 1 (inclusive)\n    :raises TypeError: If random_state is not an integer\n    :raises ValueError: If val_size + test_size is greater than 1 (i.e. the splits\n        are too large)\n    :raises ValueError: if no data_dir is provided\n    :raises AssertinoError: if the dataset sizes do not match up after reading in\n        the data from the CSV files\n\n    \"\"\"\n    if not isinstance(batch_size, int) or batch_size &lt; 1:\n        raise TypeError(\"batch_size must be a positive integer\")\n    if not isinstance(val_size, (int, float)) or val_size &lt;= 0 or val_size &gt;= 1:\n        raise TypeError(\"val_size must be a float between 0 and 1 (inclusive)\")\n    if not isinstance(test_size, (int, float)) or test_size &lt;= 0 or test_size &gt;= 1:\n        raise TypeError(\"test_size must be a float between 0 and 1 (inclusive)\")\n    if not isinstance(random_state, int):\n        raise TypeError(\"random_state must be an integer\")\n    if data_dir_path is None:\n        raise ValueError(\"data_dir_path must be provided\")\n    if test_size + val_size &gt; 1:\n        raise ValueError(\"val_size + test_size must be less than or equal to 1\")\n    if not isinstance(\n        perturbation_dataset_title, str\n    ) and perturbation_dataset_title in [\n        \"hu_reimann_tfko\",\n        \"kemmeren_tfko\",\n        \"mcisaac_oe\",\n    ]:\n        raise TypeError(\n            \"perturbation_dataset_title must be a string and must be one\"\n            \" of 'hu_reimann_tfko', 'kemmeren_tfko', or 'mcisaac_oe'\"\n        )\n\n    super().__init__()\n    self.batch_size = batch_size\n    self.val_size = val_size\n    self.test_size = test_size\n    self.random_state = random_state\n    self.data_dir_path = data_dir_path\n    self.perturbation_dataset_title = perturbation_dataset_title\n\n    self.final_data_tensor: torch.Tensor = None\n    self.binding_effect_matrix: torch.Tensor | None = None\n    self.perturbation_effect_matrix: torch.Tensor | None = None\n    self.val_dataset: TensorDataset | None = None\n    self.test_dataset: TensorDataset | None = None\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.prepare_data","title":"<code>prepare_data()</code>","text":"<p>This function reads in the binding data and perturbation data from the CSV files that we have for these datasets.</p> <p>It throws out any genes that are not present in both the binding and perturbation sets, and then structures the data in a way that the model expects and can use</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"\n    This function reads in the binding data and perturbation data from the CSV files\n    that we have for these datasets.\n\n    It throws out any genes that are not present in both the binding and\n    perturbation sets, and then structures the data in a way that the model expects\n    and can use\n\n    \"\"\"\n\n    brent_cc_path = os.path.join(self.data_dir_path, \"binding/brent_nf_cc\")\n    brent_nf_csv_files = [\n        f for f in os.listdir(brent_cc_path) if f.endswith(\".csv\")\n    ]\n    perturb_dataset_path = os.path.join(\n        self.data_dir_path, f\"perturbation/{self.perturbation_dataset_title}\"\n    )\n    perturb_dataset_csv_files = [\n        f for f in os.listdir(perturb_dataset_path) if f.endswith(\".csv\")\n    ]\n\n    # get a list of the genes in the binding data csvs\n    # for brent_cc (and the 3 perturb response datasets) the genes are\n    # in the same order in each csv, so it suffices to grab the target_locus_tag\n    # column from the first one\n    brent_cc_genes_ids = pd.read_csv(\n        os.path.join(brent_cc_path, brent_nf_csv_files[0])\n    )[\"target_locus_tag\"]\n    perturb_dataset_genes_ids = pd.read_csv(\n        os.path.join(perturb_dataset_path, perturb_dataset_csv_files[0])\n    )[\"target_locus_tag\"]\n\n    # Get the intersection of the genes in the binding and perturbation data\n    common_genes = set(brent_cc_genes_ids).intersection(perturb_dataset_genes_ids)\n\n    # Read in binding data from csv files\n    binding_data_effects = pd.DataFrame()\n    binding_data_pvalues = pd.DataFrame()\n    for i, file in enumerate(brent_nf_csv_files):\n        file_path = os.path.join(brent_cc_path, file)\n        df = pd.read_csv(file_path)\n\n        # only keep the genes that are in the intersection\n        # of the genes in the binding and perturbation data\n        df = df[df[\"target_locus_tag\"].isin(common_genes)]\n\n        # we need to handle duplicates now\n        # (some datasets have multiple occurrences of the same gene)\n        # we will keep the occurrence with the highest value in the 'effect' column\n        # we can do this by sorting the dataframe by the 'effect' column\n        # in descending order and keeping the fist occurrence of each gene\n        # this does require us to do some additional work later (see how we\n        # are consistently setting the index to 'target_locus_tag',\n        # this ensures all of our datasets are in the same order)\n        df = df.sort_values(\"effect\", ascending=False).drop_duplicates(\n            subset=\"target_locus_tag\", keep=\"first\"\n        )\n\n        # on the first iteration, add target_locus_tag column to the binding data\n        if i == 0:\n            binding_data_effects[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n            binding_data_pvalues[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n            binding_data_effects.set_index(\"target_locus_tag\", inplace=True)\n            binding_data_pvalues.set_index(\"target_locus_tag\", inplace=True)\n\n        binding_data_effects[file] = df.set_index(\"target_locus_tag\")[\"effect\"]\n        binding_data_pvalues[file] = df.set_index(\"target_locus_tag\")[\"pvalue\"]\n\n    # Read in perturbation data from csv files\n    perturbation_effects = pd.DataFrame()\n    perturbation_pvalues = pd.DataFrame()\n    for i, file in enumerate(perturb_dataset_csv_files):\n        file_path = os.path.join(perturb_dataset_path, file)\n        df = pd.read_csv(file_path)\n\n        # only keep the genes that are in the\n        # intersection of the genes in the binding and perturbation data\n        df = df[df[\"target_locus_tag\"].isin(common_genes)]\n\n        # handle duplicates\n        df = df.sort_values(\"effect\", ascending=False).drop_duplicates(\n            subset=\"target_locus_tag\", keep=\"first\"\n        )\n\n        # on the first iteration, add the target_locus_tag\n        # column to the perturbation data\n        if i == 0:\n            perturbation_effects[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n            perturbation_pvalues[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n            perturbation_effects.set_index(\"target_locus_tag\", inplace=True)\n            perturbation_pvalues.set_index(\"target_locus_tag\", inplace=True)\n\n        perturbation_effects[file] = df.set_index(\"target_locus_tag\")[\"effect\"]\n        perturbation_pvalues[file] = df.set_index(\"target_locus_tag\")[\"pvalue\"]\n\n    # shapes should be equal at this point\n    assert binding_data_effects.shape == perturbation_effects.shape\n    assert binding_data_pvalues.shape == perturbation_pvalues.shape\n\n    # reindex so that the rows in binding and perturb data match up\n    # (we need genes to be in the same order)\n    perturbation_effects = perturbation_effects.reindex(binding_data_effects.index)\n    perturbation_pvalues = perturbation_pvalues.reindex(binding_data_pvalues.index)\n\n    # concat the data into the shape expected by the model\n    # we need to first convert the data to tensors\n    binding_data_effects_tensor = torch.tensor(\n        binding_data_effects.values, dtype=torch.float64\n    )\n    binding_data_pvalues_tensor = torch.tensor(\n        binding_data_pvalues.values, dtype=torch.float64\n    )\n    perturbation_effects_tensor = torch.tensor(\n        perturbation_effects.values, dtype=torch.float64\n    )\n    perturbation_pvalues_tensor = torch.tensor(\n        perturbation_pvalues.values, dtype=torch.float64\n    )\n\n    # note that we no longer have a signal / noise tensor\n    # (like for the synthetic data)\n    self.final_data_tensor = torch.stack(\n        [\n            binding_data_effects_tensor,\n            binding_data_pvalues_tensor,\n            perturbation_effects_tensor,\n            perturbation_pvalues_tensor,\n        ],\n        dim=-1,\n    )\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.setup","title":"<code>setup(stage=None)</code>","text":"<p>This function runs after prepare_data finishes and is used to split the data into train, validation, and test sets It ensures that these datasets are of the correct dimensionality and size to be used by the model.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str | None</code> <p>The stage of the data setup (either \u2018fit\u2019 for training, \u2018validate\u2019 for validation, or \u2018test\u2019 for testing), unused for now as the model is not complicated enough to necessitate this</p> <code>None</code> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n    \"\"\"\n    This function runs after prepare_data finishes and is used to split the data\n    into train, validation, and test sets It ensures that these datasets are of the\n    correct dimensionality and size to be used by the model.\n\n    :param stage: The stage of the data setup (either 'fit' for training, 'validate'\n        for validation, or 'test' for testing), unused for now as the model is not\n        complicated enough to necessitate this\n    :type stage: Optional[str]\n\n    \"\"\"\n    self.binding_effect_matrix = self.final_data_tensor[:, :, 0]\n    self.perturbation_effect_matrix = self.final_data_tensor[:, :, 2]\n\n    # split into train, val, and test\n    X_train, X_temp, Y_train, Y_temp = train_test_split(\n        self.binding_effect_matrix,\n        self.perturbation_effect_matrix,\n        test_size=(self.val_size + self.test_size),\n        random_state=self.random_state,\n    )\n\n    # normalize test_size so that it is a percentage of the remaining data\n    self.test_size = self.test_size / (self.val_size + self.test_size)\n    X_val, X_test, Y_val, Y_test = train_test_split(\n        X_temp, Y_temp, test_size=self.test_size, random_state=self.random_state\n    )\n\n    # Convert to tensors\n    X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n        Y_train, dtype=torch.float32\n    )\n    X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n        Y_val, dtype=torch.float32\n    )\n    X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n        Y_test, dtype=torch.float32\n    )\n\n    # Set our datasets\n    self.train_dataset = TensorDataset(X_train, Y_train)\n    self.val_dataset = TensorDataset(X_val, Y_val)\n    self.test_dataset = TensorDataset(X_test, Y_test)\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Function to return the testing dataloader.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The testing dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the testing dataloader.\n\n    :return: The testing dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=False,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Function to return the training dataloader, we shuffle to avoid learning based on the order of the data.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The training dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the training dataloader, we shuffle to avoid learning based\n    on the order of the data.\n\n    :return: The training dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=True,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Function to return the validation dataloader.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The validation dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the validation dataloader.\n\n    :return: The validation dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=False,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/","title":"Synthetic Data Loader","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>A class for a synthetic data loader that generates synthetic bindiing &amp; perturbation effect data for training, validation, and testing a model This class contains all of the logic for generating and parsing the synthetic data, as well as splitting it into train, validation, and test sets It is a subclass of pytorch_lightning.LightningDataModule, which is similar to a regular PyTorch DataLoader but with added functionality for data loading.</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>class SyntheticDataLoader(LightningDataModule):\n    \"\"\"A class for a synthetic data loader that generates synthetic bindiing &amp;\n    perturbation effect data for training, validation, and testing a model This class\n    contains all of the logic for generating and parsing the synthetic data, as well as\n    splitting it into train, validation, and test sets It is a subclass of\n    pytorch_lightning.LightningDataModule, which is similar to a regular PyTorch\n    DataLoader but with added functionality for data loading.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 32,\n        num_genes: int = 1000,\n        signal: list[float] = [0.1, 0.2, 0.2, 0.4, 0.5],\n        signal_mean: float = 3.0,\n        n_sample: list[int] = [1, 2, 2, 4, 4],\n        val_size: float = 0.1,\n        test_size: float = 0.1,\n        random_state: int = 42,\n        max_mean_adjustment: float = 0.0,\n        adjustment_function: Callable[\n            [torch.Tensor, float, float, float], torch.Tensor\n        ] = default_perturbation_effect_adjustment_function,\n        tf_relationships: dict[int, list[int] | list[Relation]] = {},\n    ) -&gt; None:\n        \"\"\"\n        Constructor of SyntheticDataLoader.\n\n        :param batch_size: The number of samples in each mini-batch\n        :type batch_size: int\n        :param num_genes: The number of genes in the synthetic data (this is the number\n            of datapoints in our dataset)\n        :type num_genes: int\n        :param signal: The proportion of genes in each sample group that are put in the\n            signal grop (i.e. have a non-zero binding effect and expression response)\n        :type signal: List[int]\n        :param n_sample: The number of samples to draw from each signal group\n        :type n_sample: List[int]\n        :param val_size: The proportion of the dataset to include in the validation\n            split\n        :type val_size: float\n        :param test_size: The proportion of the dataset to include in the test split\n        :type test_size: float\n        :param random_state: The random seed to use for splitting the data (keep this\n            consistent to ensure reproduceability)\n        :type random_state: int\n        :param signal_mean: The mean of the signal distribution\n        :type signal_mean: float\n        :param max_mean_adjustment: The maximum mean adjustment to apply to the mean\n                                    of the signal (bound) perturbation effects\n        :type max_mean_adjustment: float\n        :param adjustment_function: A function that adjusts the mean of the signal\n                                    (bound) perturbation effects\n        :type adjustment_function: Callable[[torch.Tensor, float, float,\n                                   float, dict[int, list[int]]], torch.Tensor]\n        :raises TypeError: If batch_size is not an positive integer\n        :raises TypeError: If num_genes is not an positive integer\n        :raises TypeError: If signal is not a list of integers or floats\n        :raises TypeError: If n_sample is not a list of integers\n        :raises TypeError: If val_size is not a float between 0 and 1 (inclusive)\n        :raises TypeError: If test_size is not a float between 0 and 1 (inclusive)\n        :raises TypeError: If random_state is not an integer\n        :raises TypeError: If signal_mean is not a float\n        :raises ValueError: If val_size + test_size is greater than 1 (i.e. the splits\n            are too large)\n\n        \"\"\"\n        if not isinstance(batch_size, int) or batch_size &lt; 1:\n            raise TypeError(\"batch_size must be a positive integer\")\n        if not isinstance(num_genes, int) or num_genes &lt; 1:\n            raise TypeError(\"num_genes must be a positive integer\")\n        if not isinstance(signal, list) or not all(\n            isinstance(x, (int, float)) for x in signal\n        ):\n            raise TypeError(\"signal must be a list of integers or floats\")\n        if not isinstance(n_sample, list) or not all(\n            isinstance(x, int) for x in n_sample\n        ):\n            raise TypeError(\"n_sample must be a list of integers\")\n        if not isinstance(val_size, (int, float)) or val_size &lt;= 0 or val_size &gt;= 1:\n            raise TypeError(\"val_size must be a float between 0 and 1 (inclusive)\")\n        if not isinstance(test_size, (int, float)) or test_size &lt;= 0 or test_size &gt;= 1:\n            raise TypeError(\"test_size must be a float between 0 and 1 (inclusive)\")\n        if not isinstance(random_state, int):\n            raise TypeError(\"random_state must be an integer\")\n        if not isinstance(signal_mean, float):\n            raise TypeError(\"signal_mean must be a float\")\n        if test_size + val_size &gt; 1:\n            raise ValueError(\"val_size + test_size must be less than or equal to 1\")\n\n        super().__init__()\n        self.batch_size = batch_size\n        self.num_genes = num_genes\n        self.signal_mean = signal_mean\n        self.signal = signal or [0.1, 0.15, 0.2, 0.25, 0.3]\n        self.n_sample = n_sample or [1 for _ in range(len(self.signal))]\n        self.num_tfs = sum(self.n_sample)  # sum of all n_sample is the number of TFs\n        self.val_size = val_size\n        self.test_size = test_size\n        self.random_state = random_state\n\n        self.max_mean_adjustment = max_mean_adjustment\n        self.adjustment_function = adjustment_function\n        self.tf_relationships = tf_relationships\n\n        self.final_data_tensor: torch.Tensor = None\n        self.binding_effect_matrix: torch.Tensor | None = None\n        self.perturbation_effect_matrix: torch.Tensor | None = None\n        self.val_dataset: TensorDataset | None = None\n        self.test_dataset: TensorDataset | None = None\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"Function to generate the raw synthetic data and save it in a tensor For\n        explanations of the functions used to generate the data, see the\n        generate_in_silico_data tutorial notebook in the docs No assertion checks are\n        performed as that is handled in the functions in generate_data.py.\"\"\"\n        # this will be a list of length 10 with a GenePopulation object in each element\n        gene_populations_list = []\n        for signal_proportion, n_draws in zip(self.signal, self.n_sample):\n            for _ in range(n_draws):\n                gene_populations_list.append(\n                    generate_gene_population(self.num_genes, signal_proportion)\n                )\n\n        # Generate binding data for each gene population\n        binding_effect_list = [\n            generate_binding_effects(gene_population)\n            for gene_population in gene_populations_list\n        ]\n\n        # Calculate p-values for binding data\n        binding_pvalue_list = [\n            generate_pvalues(binding_data) for binding_data in binding_effect_list\n        ]\n\n        binding_data_combined = [\n            torch.stack((gene_population.labels, binding_effect, binding_pval), dim=1)\n            for gene_population, binding_effect, binding_pval in zip(\n                gene_populations_list, binding_effect_list, binding_pvalue_list\n            )\n        ]\n\n        # Stack along a new dimension (dim=1) to create a tensor of shape\n        # [num_genes, num_TFs, 3]\n        binding_data_tensor = torch.stack(binding_data_combined, dim=1)\n\n        # if we are using a mean adjustment, we need to generate perturbation\n        # effects in a slightly different way than if we are not using\n        # a mean adjustment\n        if self.max_mean_adjustment &gt; 0:\n            perturbation_effects_list = generate_perturbation_effects(\n                binding_data_tensor,\n                signal_mean=self.signal_mean,\n                tf_index=0,  # unused\n                max_mean_adjustment=self.max_mean_adjustment,\n                adjustment_function=self.adjustment_function,\n                tf_relationships=self.tf_relationships,\n            )\n\n            perturbation_pvalue_list = torch.zeros_like(perturbation_effects_list)\n            for col_index in range(perturbation_effects_list.shape[1]):\n                perturbation_pvalue_list[:, col_index] = generate_pvalues(\n                    perturbation_effects_list[:, col_index]\n                )\n\n            # take absolute values\n            perturbation_effects_list = torch.abs(perturbation_effects_list)\n\n            perturbation_effects_tensor = perturbation_effects_list\n            perturbation_pvalues_tensor = perturbation_pvalue_list\n        else:\n            perturbation_effects_list = [\n                generate_perturbation_effects(\n                    binding_data_tensor[:, tf_index, :].unsqueeze(1),\n                    signal_mean=self.signal_mean,\n                    tf_index=0,  # unused\n                )\n                for tf_index in range(sum(self.n_sample))\n            ]\n            perturbation_pvalue_list = [\n                generate_pvalues(perturbation_effects)\n                for perturbation_effects in perturbation_effects_list\n            ]\n\n            # take absolute values\n            perturbation_effects_list = [\n                torch.abs(perturbation_effects)\n                for perturbation_effects in perturbation_effects_list\n            ]\n\n            # Convert lists to tensors\n            perturbation_effects_tensor = torch.stack(perturbation_effects_list, dim=1)\n            perturbation_pvalues_tensor = torch.stack(perturbation_pvalue_list, dim=1)\n\n        # Ensure perturbation data is reshaped to match [n_genes, n_tfs]\n        # This step might need adjustment based on the actual shapes of your tensors.\n        perturbation_effects_tensor = perturbation_effects_tensor.unsqueeze(\n            -1\n        )  # Adds an extra dimension for concatenation\n        perturbation_pvalues_tensor = perturbation_pvalues_tensor.unsqueeze(\n            -1\n        )  # Adds an extra dimension for concatenation\n\n        # Concatenate along the last dimension to form a [n_genes, n_tfs, 5] tensor\n        self.final_data_tensor = torch.cat(\n            (\n                binding_data_tensor,\n                perturbation_effects_tensor,\n                perturbation_pvalues_tensor,\n            ),\n            dim=2,\n        )\n\n    def setup(self, stage: str | None = None) -&gt; None:\n        \"\"\"\n        This function runs after prepare_data finishes and is used to split the data\n        into train, validation, and test sets It ensures that these datasets are of the\n        correct dimensionality and size to be used by the model.\n\n        :param stage: The stage of the data setup (either 'fit' for training, 'validate'\n            for validation, or 'test' for testing), unused for now as the model is not\n            complicated enough to necessitate this\n        :type stage: Optional[str]\n\n        \"\"\"\n        self.binding_effect_matrix = self.final_data_tensor[:, :, 1]\n        self.perturbation_effect_matrix = self.final_data_tensor[:, :, 3]\n\n        # split into train, val, and test\n        X_train, X_temp, Y_train, Y_temp = train_test_split(\n            self.binding_effect_matrix,\n            self.perturbation_effect_matrix,\n            test_size=(self.val_size + self.test_size),\n            random_state=self.random_state,\n        )\n\n        # normalize test_size so that it is a percentage of the remaining data\n        self.test_size = self.test_size / (self.val_size + self.test_size)\n        X_val, X_test, Y_val, Y_test = train_test_split(\n            X_temp, Y_temp, test_size=self.test_size, random_state=self.random_state\n        )\n\n        # Convert to tensors\n        X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n            Y_train, dtype=torch.float32\n        )\n        X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n            Y_val, dtype=torch.float32\n        )\n        X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n            Y_test, dtype=torch.float32\n        )\n\n        # Set our datasets\n        self.train_dataset = TensorDataset(X_train, Y_train)\n        self.val_dataset = TensorDataset(X_val, Y_val)\n        self.test_dataset = TensorDataset(X_test, Y_test)\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the training dataloader, we shuffle to avoid learning based\n        on the order of the data.\n\n        :return: The training dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=True,\n            persistent_workers=True,\n        )\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the validation dataloader.\n\n        :return: The validation dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=False,\n            persistent_workers=True,\n        )\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the testing dataloader.\n\n        :return: The testing dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=False,\n            persistent_workers=True,\n        )\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.__init__","title":"<code>__init__(batch_size=32, num_genes=1000, signal=[0.1, 0.2, 0.2, 0.4, 0.5], signal_mean=3.0, n_sample=[1, 2, 2, 4, 4], val_size=0.1, test_size=0.1, random_state=42, max_mean_adjustment=0.0, adjustment_function=default_perturbation_effect_adjustment_function, tf_relationships={})</code>","text":"<p>Constructor of SyntheticDataLoader.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The number of samples in each mini-batch</p> <code>32</code> <code>num_genes</code> <code>int</code> <p>The number of genes in the synthetic data (this is the number of datapoints in our dataset)</p> <code>1000</code> <code>signal</code> <code>list[float]</code> <p>The proportion of genes in each sample group that are put in the signal grop (i.e. have a non-zero binding effect and expression response)</p> <code>[0.1, 0.2, 0.2, 0.4, 0.5]</code> <code>n_sample</code> <code>list[int]</code> <p>The number of samples to draw from each signal group</p> <code>[1, 2, 2, 4, 4]</code> <code>val_size</code> <code>float</code> <p>The proportion of the dataset to include in the validation split</p> <code>0.1</code> <code>test_size</code> <code>float</code> <p>The proportion of the dataset to include in the test split</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>The random seed to use for splitting the data (keep this consistent to ensure reproduceability)</p> <code>42</code> <code>signal_mean</code> <code>float</code> <p>The mean of the signal distribution</p> <code>3.0</code> <code>max_mean_adjustment</code> <code>float</code> <p>The maximum mean adjustment to apply to the mean of the signal (bound) perturbation effects</p> <code>0.0</code> <code>adjustment_function</code> <code>Callable[[Tensor, float, float, float], Tensor]</code> <p>A function that adjusts the mean of the signal (bound) perturbation effects</p> <code>default_perturbation_effect_adjustment_function</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If batch_size is not an positive integer</p> <code>TypeError</code> <p>If num_genes is not an positive integer</p> <code>TypeError</code> <p>If signal is not a list of integers or floats</p> <code>TypeError</code> <p>If n_sample is not a list of integers</p> <code>TypeError</code> <p>If val_size is not a float between 0 and 1 (inclusive)</p> <code>TypeError</code> <p>If test_size is not a float between 0 and 1 (inclusive)</p> <code>TypeError</code> <p>If random_state is not an integer</p> <code>TypeError</code> <p>If signal_mean is not a float</p> <code>ValueError</code> <p>If val_size + test_size is greater than 1 (i.e. the splits are too large)</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 32,\n    num_genes: int = 1000,\n    signal: list[float] = [0.1, 0.2, 0.2, 0.4, 0.5],\n    signal_mean: float = 3.0,\n    n_sample: list[int] = [1, 2, 2, 4, 4],\n    val_size: float = 0.1,\n    test_size: float = 0.1,\n    random_state: int = 42,\n    max_mean_adjustment: float = 0.0,\n    adjustment_function: Callable[\n        [torch.Tensor, float, float, float], torch.Tensor\n    ] = default_perturbation_effect_adjustment_function,\n    tf_relationships: dict[int, list[int] | list[Relation]] = {},\n) -&gt; None:\n    \"\"\"\n    Constructor of SyntheticDataLoader.\n\n    :param batch_size: The number of samples in each mini-batch\n    :type batch_size: int\n    :param num_genes: The number of genes in the synthetic data (this is the number\n        of datapoints in our dataset)\n    :type num_genes: int\n    :param signal: The proportion of genes in each sample group that are put in the\n        signal grop (i.e. have a non-zero binding effect and expression response)\n    :type signal: List[int]\n    :param n_sample: The number of samples to draw from each signal group\n    :type n_sample: List[int]\n    :param val_size: The proportion of the dataset to include in the validation\n        split\n    :type val_size: float\n    :param test_size: The proportion of the dataset to include in the test split\n    :type test_size: float\n    :param random_state: The random seed to use for splitting the data (keep this\n        consistent to ensure reproduceability)\n    :type random_state: int\n    :param signal_mean: The mean of the signal distribution\n    :type signal_mean: float\n    :param max_mean_adjustment: The maximum mean adjustment to apply to the mean\n                                of the signal (bound) perturbation effects\n    :type max_mean_adjustment: float\n    :param adjustment_function: A function that adjusts the mean of the signal\n                                (bound) perturbation effects\n    :type adjustment_function: Callable[[torch.Tensor, float, float,\n                               float, dict[int, list[int]]], torch.Tensor]\n    :raises TypeError: If batch_size is not an positive integer\n    :raises TypeError: If num_genes is not an positive integer\n    :raises TypeError: If signal is not a list of integers or floats\n    :raises TypeError: If n_sample is not a list of integers\n    :raises TypeError: If val_size is not a float between 0 and 1 (inclusive)\n    :raises TypeError: If test_size is not a float between 0 and 1 (inclusive)\n    :raises TypeError: If random_state is not an integer\n    :raises TypeError: If signal_mean is not a float\n    :raises ValueError: If val_size + test_size is greater than 1 (i.e. the splits\n        are too large)\n\n    \"\"\"\n    if not isinstance(batch_size, int) or batch_size &lt; 1:\n        raise TypeError(\"batch_size must be a positive integer\")\n    if not isinstance(num_genes, int) or num_genes &lt; 1:\n        raise TypeError(\"num_genes must be a positive integer\")\n    if not isinstance(signal, list) or not all(\n        isinstance(x, (int, float)) for x in signal\n    ):\n        raise TypeError(\"signal must be a list of integers or floats\")\n    if not isinstance(n_sample, list) or not all(\n        isinstance(x, int) for x in n_sample\n    ):\n        raise TypeError(\"n_sample must be a list of integers\")\n    if not isinstance(val_size, (int, float)) or val_size &lt;= 0 or val_size &gt;= 1:\n        raise TypeError(\"val_size must be a float between 0 and 1 (inclusive)\")\n    if not isinstance(test_size, (int, float)) or test_size &lt;= 0 or test_size &gt;= 1:\n        raise TypeError(\"test_size must be a float between 0 and 1 (inclusive)\")\n    if not isinstance(random_state, int):\n        raise TypeError(\"random_state must be an integer\")\n    if not isinstance(signal_mean, float):\n        raise TypeError(\"signal_mean must be a float\")\n    if test_size + val_size &gt; 1:\n        raise ValueError(\"val_size + test_size must be less than or equal to 1\")\n\n    super().__init__()\n    self.batch_size = batch_size\n    self.num_genes = num_genes\n    self.signal_mean = signal_mean\n    self.signal = signal or [0.1, 0.15, 0.2, 0.25, 0.3]\n    self.n_sample = n_sample or [1 for _ in range(len(self.signal))]\n    self.num_tfs = sum(self.n_sample)  # sum of all n_sample is the number of TFs\n    self.val_size = val_size\n    self.test_size = test_size\n    self.random_state = random_state\n\n    self.max_mean_adjustment = max_mean_adjustment\n    self.adjustment_function = adjustment_function\n    self.tf_relationships = tf_relationships\n\n    self.final_data_tensor: torch.Tensor = None\n    self.binding_effect_matrix: torch.Tensor | None = None\n    self.perturbation_effect_matrix: torch.Tensor | None = None\n    self.val_dataset: TensorDataset | None = None\n    self.test_dataset: TensorDataset | None = None\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Function to generate the raw synthetic data and save it in a tensor For explanations of the functions used to generate the data, see the generate_in_silico_data tutorial notebook in the docs No assertion checks are performed as that is handled in the functions in generate_data.py.</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"Function to generate the raw synthetic data and save it in a tensor For\n    explanations of the functions used to generate the data, see the\n    generate_in_silico_data tutorial notebook in the docs No assertion checks are\n    performed as that is handled in the functions in generate_data.py.\"\"\"\n    # this will be a list of length 10 with a GenePopulation object in each element\n    gene_populations_list = []\n    for signal_proportion, n_draws in zip(self.signal, self.n_sample):\n        for _ in range(n_draws):\n            gene_populations_list.append(\n                generate_gene_population(self.num_genes, signal_proportion)\n            )\n\n    # Generate binding data for each gene population\n    binding_effect_list = [\n        generate_binding_effects(gene_population)\n        for gene_population in gene_populations_list\n    ]\n\n    # Calculate p-values for binding data\n    binding_pvalue_list = [\n        generate_pvalues(binding_data) for binding_data in binding_effect_list\n    ]\n\n    binding_data_combined = [\n        torch.stack((gene_population.labels, binding_effect, binding_pval), dim=1)\n        for gene_population, binding_effect, binding_pval in zip(\n            gene_populations_list, binding_effect_list, binding_pvalue_list\n        )\n    ]\n\n    # Stack along a new dimension (dim=1) to create a tensor of shape\n    # [num_genes, num_TFs, 3]\n    binding_data_tensor = torch.stack(binding_data_combined, dim=1)\n\n    # if we are using a mean adjustment, we need to generate perturbation\n    # effects in a slightly different way than if we are not using\n    # a mean adjustment\n    if self.max_mean_adjustment &gt; 0:\n        perturbation_effects_list = generate_perturbation_effects(\n            binding_data_tensor,\n            signal_mean=self.signal_mean,\n            tf_index=0,  # unused\n            max_mean_adjustment=self.max_mean_adjustment,\n            adjustment_function=self.adjustment_function,\n            tf_relationships=self.tf_relationships,\n        )\n\n        perturbation_pvalue_list = torch.zeros_like(perturbation_effects_list)\n        for col_index in range(perturbation_effects_list.shape[1]):\n            perturbation_pvalue_list[:, col_index] = generate_pvalues(\n                perturbation_effects_list[:, col_index]\n            )\n\n        # take absolute values\n        perturbation_effects_list = torch.abs(perturbation_effects_list)\n\n        perturbation_effects_tensor = perturbation_effects_list\n        perturbation_pvalues_tensor = perturbation_pvalue_list\n    else:\n        perturbation_effects_list = [\n            generate_perturbation_effects(\n                binding_data_tensor[:, tf_index, :].unsqueeze(1),\n                signal_mean=self.signal_mean,\n                tf_index=0,  # unused\n            )\n            for tf_index in range(sum(self.n_sample))\n        ]\n        perturbation_pvalue_list = [\n            generate_pvalues(perturbation_effects)\n            for perturbation_effects in perturbation_effects_list\n        ]\n\n        # take absolute values\n        perturbation_effects_list = [\n            torch.abs(perturbation_effects)\n            for perturbation_effects in perturbation_effects_list\n        ]\n\n        # Convert lists to tensors\n        perturbation_effects_tensor = torch.stack(perturbation_effects_list, dim=1)\n        perturbation_pvalues_tensor = torch.stack(perturbation_pvalue_list, dim=1)\n\n    # Ensure perturbation data is reshaped to match [n_genes, n_tfs]\n    # This step might need adjustment based on the actual shapes of your tensors.\n    perturbation_effects_tensor = perturbation_effects_tensor.unsqueeze(\n        -1\n    )  # Adds an extra dimension for concatenation\n    perturbation_pvalues_tensor = perturbation_pvalues_tensor.unsqueeze(\n        -1\n    )  # Adds an extra dimension for concatenation\n\n    # Concatenate along the last dimension to form a [n_genes, n_tfs, 5] tensor\n    self.final_data_tensor = torch.cat(\n        (\n            binding_data_tensor,\n            perturbation_effects_tensor,\n            perturbation_pvalues_tensor,\n        ),\n        dim=2,\n    )\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.setup","title":"<code>setup(stage=None)</code>","text":"<p>This function runs after prepare_data finishes and is used to split the data into train, validation, and test sets It ensures that these datasets are of the correct dimensionality and size to be used by the model.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str | None</code> <p>The stage of the data setup (either \u2018fit\u2019 for training, \u2018validate\u2019 for validation, or \u2018test\u2019 for testing), unused for now as the model is not complicated enough to necessitate this</p> <code>None</code> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n    \"\"\"\n    This function runs after prepare_data finishes and is used to split the data\n    into train, validation, and test sets It ensures that these datasets are of the\n    correct dimensionality and size to be used by the model.\n\n    :param stage: The stage of the data setup (either 'fit' for training, 'validate'\n        for validation, or 'test' for testing), unused for now as the model is not\n        complicated enough to necessitate this\n    :type stage: Optional[str]\n\n    \"\"\"\n    self.binding_effect_matrix = self.final_data_tensor[:, :, 1]\n    self.perturbation_effect_matrix = self.final_data_tensor[:, :, 3]\n\n    # split into train, val, and test\n    X_train, X_temp, Y_train, Y_temp = train_test_split(\n        self.binding_effect_matrix,\n        self.perturbation_effect_matrix,\n        test_size=(self.val_size + self.test_size),\n        random_state=self.random_state,\n    )\n\n    # normalize test_size so that it is a percentage of the remaining data\n    self.test_size = self.test_size / (self.val_size + self.test_size)\n    X_val, X_test, Y_val, Y_test = train_test_split(\n        X_temp, Y_temp, test_size=self.test_size, random_state=self.random_state\n    )\n\n    # Convert to tensors\n    X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n        Y_train, dtype=torch.float32\n    )\n    X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n        Y_val, dtype=torch.float32\n    )\n    X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n        Y_test, dtype=torch.float32\n    )\n\n    # Set our datasets\n    self.train_dataset = TensorDataset(X_train, Y_train)\n    self.val_dataset = TensorDataset(X_val, Y_val)\n    self.test_dataset = TensorDataset(X_test, Y_test)\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Function to return the testing dataloader.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The testing dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the testing dataloader.\n\n    :return: The testing dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=False,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Function to return the training dataloader, we shuffle to avoid learning based on the order of the data.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The training dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the training dataloader, we shuffle to avoid learning based\n    on the order of the data.\n\n    :return: The training dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=True,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Function to return the validation dataloader.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The validation dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the validation dataloader.\n\n    :return: The validation dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=False,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"ml_models/customizable_model/","title":"Models","text":"<p>             Bases: <code>LightningModule</code></p> <p>A class for a customizable model that takes in binding effects for each transcription factor and predicts gene expression values This class contains all of the logic for setup, training, validation, and testing of the model, as well as defining how data is passed through the model It is a subclass of pytorch_lightning.LightningModule, which is similar to a regular PyTorch nn.module but with added functionality for training and validation.</p> <p>This model takes in many more parameters that SimpleModel, allowing us to experiement with many hyperparameter and architecture choices in order to decide what is best for our task &amp; data</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>class CustomizableModel(pl.LightningModule):\n    \"\"\"\n    A class for a customizable model that takes in binding effects for each\n    transcription factor and predicts gene expression values This class contains all of\n    the logic for setup, training, validation, and testing of the model, as well as\n    defining how data is passed through the model It is a subclass of\n    pytorch_lightning.LightningModule, which is similar to a regular PyTorch nn.module\n    but with added functionality for training and validation.\n\n    This model takes in many more parameters that SimpleModel, allowing us to\n    experiement with many hyperparameter and architecture choices in order to decide\n    what is best for our task &amp; data\n\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        output_dim: int,\n        lr: float = 0.001,\n        hidden_layer_num: int = 1,\n        hidden_layer_sizes: list = [128],\n        activation: str = \"ReLU\",  # can be \"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\"\n        optimizer: str = \"Adam\",  # can be \"Adam\", \"SGD\", \"RMSprop\"\n        L2_regularization_term: float = 0.0,\n        dropout_rate: float = 0.0,\n    ) -&gt; None:\n        \"\"\"\n        Constructor of CustomizableModel.\n\n        :param input_dim: The number of input features to our model, these are the\n            binding effects for each transcription factor for a specific gene\n        :type input_dim: int\n        :param output_dim: The number of output features of our model, this is the\n            predicted gene expression value for each TF\n        :type output_dim: int\n        :param lr: The learning rate for the optimizer\n        :type lr: float\n        :raises TypeError: If input_dim is not an integer\n        :raises TypeError: If output_dim is not an integer\n        :raises TypeError: If lr is not a positive float\n        :raises ValueError: If input_dim or output_dim are not positive\n        :param hidden_layer_num: The number of hidden layers in the model\n        :type hidden_layer_num: int\n        :param hidden_layer_sizes: The size of each hidden layer in the model\n        :type hidden_layer_sizes: list\n\n        \"\"\"\n        if not isinstance(input_dim, int):\n            raise TypeError(\"input_dim must be an integer\")\n        if not isinstance(output_dim, int):\n            raise TypeError(\"output_dim must be an integer\")\n        if not isinstance(lr, float) or lr &lt;= 0:\n            raise TypeError(\"lr must be a positive float\")\n        if input_dim &lt; 1 or output_dim &lt; 1:\n            raise ValueError(\"input_dim and output_dim must be positive integers\")\n        if not isinstance(hidden_layer_num, int):\n            raise TypeError(\"hidden_layer_num must be an integer\")\n        if not isinstance(hidden_layer_sizes, list) or not all(\n            isinstance(i, int) for i in hidden_layer_sizes\n        ):\n            raise TypeError(\"hidden_layer_sizes must be a list of integers\")\n        if len(hidden_layer_sizes) != hidden_layer_num:\n            raise ValueError(\n                \"hidden_layer_sizes must have length equal to hidden_layer_num\"\n            )\n        if not isinstance(activation, str) or activation not in [\n            \"ReLU\",\n            \"Sigmoid\",\n            \"Tanh\",\n            \"LeakyReLU\",\n        ]:\n            raise ValueError(\n                \"activation must be one of 'ReLU', 'Sigmoid', 'Tanh', 'LeakyReLU'\"\n            )\n        if not isinstance(optimizer, str) or optimizer not in [\n            \"Adam\",\n            \"SGD\",\n            \"RMSprop\",\n        ]:\n            raise ValueError(\"optimizer must be one of 'Adam', 'SGD', 'RMSprop'\")\n        if not isinstance(L2_regularization_term, float) or L2_regularization_term &lt; 0:\n            raise TypeError(\"L2_regularization_term must be a non-negative float\")\n        if not isinstance(dropout_rate, float) or dropout_rate &lt; 0 or dropout_rate &gt; 1:\n            raise TypeError(\"dropout_rate must be a float between 0 and 1\")\n\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.lr = lr\n        self.hidden_layer_num = hidden_layer_num\n        self.hidden_layer_sizes = hidden_layer_sizes\n        self.optimizer = optimizer\n        self.L2_regularization_term = L2_regularization_term\n        self.save_hyperparameters()\n\n        match activation:\n            case \"ReLU\":\n                self.activation = nn.ReLU()\n            case \"Sigmoid\":\n                self.activation = nn.Sigmoid()\n            case \"Tanh\":\n                self.activation = nn.Tanh()\n            case \"LeakyReLU\":\n                self.activation = nn.LeakyReLU()\n            case _:\n                raise ValueError(\n                    \"activation must be one of 'ReLU', 'Sigmoid', 'Tanh', 'LeakyReLU'\"\n                )\n\n        self.input_layer = nn.Linear(input_dim, hidden_layer_sizes[0])\n        self.hidden_layers = nn.ModuleList([])\n        for i in range(hidden_layer_num - 1):\n            self.hidden_layers.append(\n                nn.Linear(hidden_layer_sizes[i], hidden_layer_sizes[i + 1])\n            )\n        self.output_layer = nn.Linear(hidden_layer_sizes[-1], output_dim)\n\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n        self.mae = MeanAbsoluteError()\n        self.SMSE = SMSE()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the model (i.e. how predictions are made for a given input)\n\n        :param x: The input data to the model (minus the target y values)\n        :type x: torch.Tensor\n        :return: The predicted y values for the input x, this is a tensor of shape\n            (batch_size, output_dim)\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x = self.dropout(self.activation(self.input_layer(x)))\n        for hidden_layer in self.hidden_layers:\n            x = self.dropout(self.activation(hidden_layer(x)))\n        x = self.output_layer(x)\n        return x\n\n    def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Training step for the model, this is called for each batch of data during\n        training.\n\n        :param batch: The batch of data to train on\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the training batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        mse_loss = nn.functional.mse_loss(y_pred, y)\n        self.log(\"train_mse\", mse_loss)\n        self.log(\"train_mae\", self.mae(y_pred, y))\n        self.log(\"train_smse\", self.SMSE(y_pred, y))\n        return mse_loss\n\n    def validation_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Validation step for the model, this is called for each batch of data during\n        validation.\n\n        :param batch: The batch of data to validate on\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the validation batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        mse_loss = nn.functional.mse_loss(y_pred, y)\n        self.log(\"val_mse\", mse_loss)\n        self.log(\"val_mae\", self.mae(y_pred, y))\n        self.log(\"val_smse\", self.SMSE(y_pred, y))\n        return mse_loss\n\n    def test_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Test step for the model, this is called for each batch of data during testing\n        Testing is only performed after training and validation when we have chosen a\n        final model We want to test our final model on unseen data (which is why we use\n        validation sets to \"test\" during training)\n\n        :param batch: The batch of data to test on (this will have size (batch_size,\n            input_dim)\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the test batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        mse_loss = nn.functional.mse_loss(y_pred, y)\n        self.log(\"test_mse\", mse_loss)\n        self.log(\"test_mae\", self.mae(y_pred, y))\n        self.log(\"test_smse\", self.SMSE(y_pred, y))\n        return mse_loss\n\n    def configure_optimizers(self) -&gt; Optimizer:\n        \"\"\"\n        Configure the optimizer for the model.\n\n        :return: The optimizer for the model\n        :rtype: Optimizer\n\n        \"\"\"\n        match self.optimizer:\n            case \"Adam\":\n                return torch.optim.Adam(\n                    self.parameters(),\n                    lr=self.lr,\n                    weight_decay=self.L2_regularization_term,\n                )\n            case \"SGD\":\n                return torch.optim.SGD(\n                    self.parameters(),\n                    lr=self.lr,\n                    weight_decay=self.L2_regularization_term,\n                )\n            case \"RMSprop\":\n                return torch.optim.RMSprop(\n                    self.parameters(),\n                    lr=self.lr,\n                    weight_decay=self.L2_regularization_term,\n                )\n            case _:\n                raise ValueError(\"optimizer must be one of 'Adam', 'SGD', 'RMSprop'\")\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.__init__","title":"<code>__init__(input_dim, output_dim, lr=0.001, hidden_layer_num=1, hidden_layer_sizes=[128], activation='ReLU', optimizer='Adam', L2_regularization_term=0.0, dropout_rate=0.0)</code>","text":"<p>Constructor of CustomizableModel.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>The number of input features to our model, these are the binding effects for each transcription factor for a specific gene</p> required <code>output_dim</code> <code>int</code> <p>The number of output features of our model, this is the predicted gene expression value for each TF</p> required <code>lr</code> <code>float</code> <p>The learning rate for the optimizer</p> <code>0.001</code> <code>hidden_layer_num</code> <code>int</code> <p>The number of hidden layers in the model</p> <code>1</code> <code>hidden_layer_sizes</code> <code>list</code> <p>The size of each hidden layer in the model</p> <code>[128]</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input_dim is not an integer</p> <code>TypeError</code> <p>If output_dim is not an integer</p> <code>TypeError</code> <p>If lr is not a positive float</p> <code>ValueError</code> <p>If input_dim or output_dim are not positive</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    output_dim: int,\n    lr: float = 0.001,\n    hidden_layer_num: int = 1,\n    hidden_layer_sizes: list = [128],\n    activation: str = \"ReLU\",  # can be \"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\"\n    optimizer: str = \"Adam\",  # can be \"Adam\", \"SGD\", \"RMSprop\"\n    L2_regularization_term: float = 0.0,\n    dropout_rate: float = 0.0,\n) -&gt; None:\n    \"\"\"\n    Constructor of CustomizableModel.\n\n    :param input_dim: The number of input features to our model, these are the\n        binding effects for each transcription factor for a specific gene\n    :type input_dim: int\n    :param output_dim: The number of output features of our model, this is the\n        predicted gene expression value for each TF\n    :type output_dim: int\n    :param lr: The learning rate for the optimizer\n    :type lr: float\n    :raises TypeError: If input_dim is not an integer\n    :raises TypeError: If output_dim is not an integer\n    :raises TypeError: If lr is not a positive float\n    :raises ValueError: If input_dim or output_dim are not positive\n    :param hidden_layer_num: The number of hidden layers in the model\n    :type hidden_layer_num: int\n    :param hidden_layer_sizes: The size of each hidden layer in the model\n    :type hidden_layer_sizes: list\n\n    \"\"\"\n    if not isinstance(input_dim, int):\n        raise TypeError(\"input_dim must be an integer\")\n    if not isinstance(output_dim, int):\n        raise TypeError(\"output_dim must be an integer\")\n    if not isinstance(lr, float) or lr &lt;= 0:\n        raise TypeError(\"lr must be a positive float\")\n    if input_dim &lt; 1 or output_dim &lt; 1:\n        raise ValueError(\"input_dim and output_dim must be positive integers\")\n    if not isinstance(hidden_layer_num, int):\n        raise TypeError(\"hidden_layer_num must be an integer\")\n    if not isinstance(hidden_layer_sizes, list) or not all(\n        isinstance(i, int) for i in hidden_layer_sizes\n    ):\n        raise TypeError(\"hidden_layer_sizes must be a list of integers\")\n    if len(hidden_layer_sizes) != hidden_layer_num:\n        raise ValueError(\n            \"hidden_layer_sizes must have length equal to hidden_layer_num\"\n        )\n    if not isinstance(activation, str) or activation not in [\n        \"ReLU\",\n        \"Sigmoid\",\n        \"Tanh\",\n        \"LeakyReLU\",\n    ]:\n        raise ValueError(\n            \"activation must be one of 'ReLU', 'Sigmoid', 'Tanh', 'LeakyReLU'\"\n        )\n    if not isinstance(optimizer, str) or optimizer not in [\n        \"Adam\",\n        \"SGD\",\n        \"RMSprop\",\n    ]:\n        raise ValueError(\"optimizer must be one of 'Adam', 'SGD', 'RMSprop'\")\n    if not isinstance(L2_regularization_term, float) or L2_regularization_term &lt; 0:\n        raise TypeError(\"L2_regularization_term must be a non-negative float\")\n    if not isinstance(dropout_rate, float) or dropout_rate &lt; 0 or dropout_rate &gt; 1:\n        raise TypeError(\"dropout_rate must be a float between 0 and 1\")\n\n    super().__init__()\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.lr = lr\n    self.hidden_layer_num = hidden_layer_num\n    self.hidden_layer_sizes = hidden_layer_sizes\n    self.optimizer = optimizer\n    self.L2_regularization_term = L2_regularization_term\n    self.save_hyperparameters()\n\n    match activation:\n        case \"ReLU\":\n            self.activation = nn.ReLU()\n        case \"Sigmoid\":\n            self.activation = nn.Sigmoid()\n        case \"Tanh\":\n            self.activation = nn.Tanh()\n        case \"LeakyReLU\":\n            self.activation = nn.LeakyReLU()\n        case _:\n            raise ValueError(\n                \"activation must be one of 'ReLU', 'Sigmoid', 'Tanh', 'LeakyReLU'\"\n            )\n\n    self.input_layer = nn.Linear(input_dim, hidden_layer_sizes[0])\n    self.hidden_layers = nn.ModuleList([])\n    for i in range(hidden_layer_num - 1):\n        self.hidden_layers.append(\n            nn.Linear(hidden_layer_sizes[i], hidden_layer_sizes[i + 1])\n        )\n    self.output_layer = nn.Linear(hidden_layer_sizes[-1], output_dim)\n\n    self.dropout = nn.Dropout(p=dropout_rate)\n\n    self.mae = MeanAbsoluteError()\n    self.SMSE = SMSE()\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure the optimizer for the model.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>The optimizer for the model</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def configure_optimizers(self) -&gt; Optimizer:\n    \"\"\"\n    Configure the optimizer for the model.\n\n    :return: The optimizer for the model\n    :rtype: Optimizer\n\n    \"\"\"\n    match self.optimizer:\n        case \"Adam\":\n            return torch.optim.Adam(\n                self.parameters(),\n                lr=self.lr,\n                weight_decay=self.L2_regularization_term,\n            )\n        case \"SGD\":\n            return torch.optim.SGD(\n                self.parameters(),\n                lr=self.lr,\n                weight_decay=self.L2_regularization_term,\n            )\n        case \"RMSprop\":\n            return torch.optim.RMSprop(\n                self.parameters(),\n                lr=self.lr,\n                weight_decay=self.L2_regularization_term,\n            )\n        case _:\n            raise ValueError(\"optimizer must be one of 'Adam', 'SGD', 'RMSprop'\")\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the model (i.e. how predictions are made for a given input)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input data to the model (minus the target y values)</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The predicted y values for the input x, this is a tensor of shape (batch_size, output_dim)</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the model (i.e. how predictions are made for a given input)\n\n    :param x: The input data to the model (minus the target y values)\n    :type x: torch.Tensor\n    :return: The predicted y values for the input x, this is a tensor of shape\n        (batch_size, output_dim)\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x = self.dropout(self.activation(self.input_layer(x)))\n    for hidden_layer in self.hidden_layers:\n        x = self.dropout(self.activation(hidden_layer(x)))\n    x = self.output_layer(x)\n    return x\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Test step for the model, this is called for each batch of data during testing Testing is only performed after training and validation when we have chosen a final model We want to test our final model on unseen data (which is why we use validation sets to \u201ctest\u201d during training)</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to test on (this will have size (batch_size, input_dim)</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the test batch</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def test_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Test step for the model, this is called for each batch of data during testing\n    Testing is only performed after training and validation when we have chosen a\n    final model We want to test our final model on unseen data (which is why we use\n    validation sets to \"test\" during training)\n\n    :param batch: The batch of data to test on (this will have size (batch_size,\n        input_dim)\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the test batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    mse_loss = nn.functional.mse_loss(y_pred, y)\n    self.log(\"test_mse\", mse_loss)\n    self.log(\"test_mae\", self.mae(y_pred, y))\n    self.log(\"test_smse\", self.SMSE(y_pred, y))\n    return mse_loss\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step for the model, this is called for each batch of data during training.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to train on</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the training batch</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Training step for the model, this is called for each batch of data during\n    training.\n\n    :param batch: The batch of data to train on\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the training batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    mse_loss = nn.functional.mse_loss(y_pred, y)\n    self.log(\"train_mse\", mse_loss)\n    self.log(\"train_mae\", self.mae(y_pred, y))\n    self.log(\"train_smse\", self.SMSE(y_pred, y))\n    return mse_loss\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step for the model, this is called for each batch of data during validation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to validate on</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the validation batch</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def validation_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Validation step for the model, this is called for each batch of data during\n    validation.\n\n    :param batch: The batch of data to validate on\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the validation batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    mse_loss = nn.functional.mse_loss(y_pred, y)\n    self.log(\"val_mse\", mse_loss)\n    self.log(\"val_mae\", self.mae(y_pred, y))\n    self.log(\"val_smse\", self.SMSE(y_pred, y))\n    return mse_loss\n</code></pre>"},{"location":"ml_models/metrics_compute_nrmse/","title":"Metrics compute nrmse","text":"<p>Compute the Normalized Root Mean Squared Error. This can be used to better compare models trained on different datasets with differnet scales, although it is not perfectly scale invariant.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>torch.Tensor</code> <p>The predicted y values</p> required <code>y_true</code> <code>torch.Tensor</code> <p>The true y values</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The normalized root mean squared error</p> Source code in <code>yeastdnnexplorer/ml_models/metrics.py</code> <pre><code>def compute_nrmse(self, y_pred, y_true):\n    \"\"\"\n    Compute the Normalized Root Mean Squared Error. This can be used to better compare\n    models trained on different datasets with differnet scales, although it is not\n    perfectly scale invariant.\n\n    :param y_pred: The predicted y values\n    :type y_pred: torch.Tensor\n    :param y_true: The true y values\n    :type y_true: torch.Tensor\n    :return: The normalized root mean squared error\n    :rtype: torch.Tensor\n\n    \"\"\"\n    rmse = torch.sqrt(F.mse_loss(y_pred, y_true))\n\n    # normalize with the range of true y values\n    y_range = y_true.max() - y_true.min()\n    nrmse = rmse / y_range\n    return nrmse\n</code></pre>"},{"location":"ml_models/metrics_smse/","title":"Metrics smse","text":"<p>             Bases: <code>Metric</code></p> <p>A class for computing the standardized mean squared error (SMSE) metric.</p> <p>This metric is defined as the mean squared error divided by the variance of the true values (the target data). Because we are dividing by the variance of the true values, this metric is scale-independent and does not depend on the mean of the true values. It allows us to effectively compare models drawn from different datasets with differring scales or means (as long as their variances are at least relatively similar)</p> Source code in <code>yeastdnnexplorer/ml_models/metrics.py</code> <pre><code>class SMSE(Metric):\n    \"\"\"\n    A class for computing the standardized mean squared error (SMSE) metric.\n\n    This metric is defined as the mean squared error divided by the variance of the true\n    values (the target data). Because we are dividing by the variance of the true\n    values, this metric is scale-independent and does not depend on the mean of the true\n    values. It allows us to effectively compare models drawn from different datasets\n    with differring scales or means (as long as their variances are at least relatively\n    similar)\n\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the SMSE metric.\"\"\"\n        super().__init__()\n        self.add_state(\"mse\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n        self.add_state(\"variance\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n        self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n    def update(self, y_pred: torch.Tensor, y_true: torch.Tensor):\n        \"\"\"\n        Update the metric with new predictions and true values.\n\n        :param y_pred: The predicted y values\n        :type y_pred: torch.Tensor\n        :param y_true: The true y values\n        :type y_true: torch.Tensor\n\n        \"\"\"\n        self.mse += F.mse_loss(y_pred, y_true, reduction=\"sum\")\n        self.variance += torch.var(y_true, unbiased=False) * y_true.size(\n            0\n        )  # Total variance (TODO should we have unbiased=False here?)\n        self.num_samples += y_true.numel()\n\n    def compute(self):\n        \"\"\"\n        Compute the SMSE metric.\n\n        :return: The SMSE metric\n        :rtype: torch.Tensor\n\n        \"\"\"\n        mean_mse = self.mse / self.num_samples\n        mean_variance = self.variance / self.num_samples\n        return mean_mse / mean_variance\n</code></pre>"},{"location":"ml_models/metrics_smse/#yeastdnnexplorer.ml_models.metrics.SMSE.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the SMSE metric.</p> Source code in <code>yeastdnnexplorer/ml_models/metrics.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the SMSE metric.\"\"\"\n    super().__init__()\n    self.add_state(\"mse\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n    self.add_state(\"variance\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n    self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"ml_models/metrics_smse/#yeastdnnexplorer.ml_models.metrics.SMSE.compute","title":"<code>compute()</code>","text":"<p>Compute the SMSE metric.</p> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The SMSE metric</p> Source code in <code>yeastdnnexplorer/ml_models/metrics.py</code> <pre><code>def compute(self):\n    \"\"\"\n    Compute the SMSE metric.\n\n    :return: The SMSE metric\n    :rtype: torch.Tensor\n\n    \"\"\"\n    mean_mse = self.mse / self.num_samples\n    mean_variance = self.variance / self.num_samples\n    return mean_mse / mean_variance\n</code></pre>"},{"location":"ml_models/metrics_smse/#yeastdnnexplorer.ml_models.metrics.SMSE.update","title":"<code>update(y_pred, y_true)</code>","text":"<p>Update the metric with new predictions and true values.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>The predicted y values</p> required <code>y_true</code> <code>Tensor</code> <p>The true y values</p> required Source code in <code>yeastdnnexplorer/ml_models/metrics.py</code> <pre><code>def update(self, y_pred: torch.Tensor, y_true: torch.Tensor):\n    \"\"\"\n    Update the metric with new predictions and true values.\n\n    :param y_pred: The predicted y values\n    :type y_pred: torch.Tensor\n    :param y_true: The true y values\n    :type y_true: torch.Tensor\n\n    \"\"\"\n    self.mse += F.mse_loss(y_pred, y_true, reduction=\"sum\")\n    self.variance += torch.var(y_true, unbiased=False) * y_true.size(\n        0\n    )  # Total variance (TODO should we have unbiased=False here?)\n    self.num_samples += y_true.numel()\n</code></pre>"},{"location":"ml_models/simple_model/","title":"Simple model","text":""},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel","title":"<code>SimpleModel</code>","text":"<p>             Bases: <code>LightningModule</code></p> <p>A class for a simple linear model that takes in binding effects for each transcription factor and predicts gene expression values This class contains all of the logic for setup, training, validation, and testing of the model, as well as defining how data is passed through the model It is a subclass of pytorch_lightning.LightningModule, which is similar to a regular PyTorch nn.module but with added functionality for training and validation.</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>class SimpleModel(pl.LightningModule):\n    \"\"\"A class for a simple linear model that takes in binding effects for each\n    transcription factor and predicts gene expression values This class contains all of\n    the logic for setup, training, validation, and testing of the model, as well as\n    defining how data is passed through the model It is a subclass of\n    pytorch_lightning.LightningModule, which is similar to a regular PyTorch nn.module\n    but with added functionality for training and validation.\"\"\"\n\n    def __init__(self, input_dim: int, output_dim: int, lr: float = 0.001) -&gt; None:\n        \"\"\"\n        Constructor of SimpleModel.\n\n        :param input_dim: The number of input features to our model, these are the\n            binding effects for each transcription factor for a specific gene\n        :type input_dim: int\n        :param output_dim: The number of output features of our model, this is the\n            predicted gene expression value for each TF\n        :type output_dim: int\n        :param lr: The learning rate for the optimizer\n        :type lr: float\n        :raises TypeError: If input_dim is not an integer\n        :raises TypeError: If output_dim is not an integer\n        :raises TypeError: If lr is not a positive float\n        :raises ValueError: If input_dim or output_dim are not positive\n\n        \"\"\"\n        if not isinstance(input_dim, int):\n            raise TypeError(\"input_dim must be an integer\")\n        if not isinstance(output_dim, int):\n            raise TypeError(\"output_dim must be an integer\")\n        if not isinstance(lr, float) or lr &lt;= 0:\n            raise TypeError(\"lr must be a positive float\")\n        if input_dim &lt; 1 or output_dim &lt; 1:\n            raise ValueError(\"input_dim and output_dim must be positive integers\")\n\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.lr = lr\n        self.save_hyperparameters()\n\n        self.mae = MeanAbsoluteError()\n        self.SMSE = SMSE()\n\n        # define layers for the model here\n        self.linear1 = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the model (i.e. how predictions are made for a given input)\n\n        :param x: The input data to the model (minus the target y values)\n        :type x: torch.Tensor\n        :return: The predicted y values for the input x, this is a tensor of shape\n            (batch_size, output_dim)\n        :rtype: torch.Tensor\n\n        \"\"\"\n        return self.linear1(x)\n\n    def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Training step for the model, this is called for each batch of data during\n        training.\n\n        :param batch: The batch of data to train on\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the training batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        loss = nn.functional.mse_loss(y_pred, y)\n        self.log(\"train_mse\", loss)\n        self.log(\"train_mae\", self.mae(y_pred, y))\n        self.log(\"train_smse\", self.SMSE(y_pred, y))\n        return loss\n\n    def validation_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Validation step for the model, this is called for each batch of data during\n        validation.\n\n        :param batch: The batch of data to validate on\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the validation batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        loss = nn.functional.mse_loss(y_pred, y)\n\n        self.log(\"val_mse\", loss)\n        self.log(\"val_mae\", self.mae(y_pred, y))\n        self.log(\"val_smse\", self.SMSE(y_pred, y))\n        return loss\n\n    def test_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Test step for the model, this is called for each batch of data during testing\n        Testing is only performed after training and validation when we have chosen a\n        final model We want to test our final model on unseen data (which is why we use\n        validation sets to \"test\" during training)\n\n        :param batch: The batch of data to test on (this will have size (batch_size,\n            input_dim)\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the test batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        loss = nn.functional.mse_loss(y_pred, y)\n        self.log(\"test_mse\", loss)\n        self.log(\"test_mae\", self.mae(y_pred, y))\n        self.log(\"test_smse\", self.SMSE(y_pred, y))\n        return loss\n\n    def configure_optimizers(self) -&gt; Optimizer:\n        \"\"\"\n        Configure the optimizer for the model.\n\n        :return: The optimizer for the model\n        :rtype: Optimizer\n\n        \"\"\"\n        return torch.optim.Adam(self.parameters(), lr=self.lr)\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.__init__","title":"<code>__init__(input_dim, output_dim, lr=0.001)</code>","text":"<p>Constructor of SimpleModel.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>The number of input features to our model, these are the binding effects for each transcription factor for a specific gene</p> required <code>output_dim</code> <code>int</code> <p>The number of output features of our model, this is the predicted gene expression value for each TF</p> required <code>lr</code> <code>float</code> <p>The learning rate for the optimizer</p> <code>0.001</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input_dim is not an integer</p> <code>TypeError</code> <p>If output_dim is not an integer</p> <code>TypeError</code> <p>If lr is not a positive float</p> <code>ValueError</code> <p>If input_dim or output_dim are not positive</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def __init__(self, input_dim: int, output_dim: int, lr: float = 0.001) -&gt; None:\n    \"\"\"\n    Constructor of SimpleModel.\n\n    :param input_dim: The number of input features to our model, these are the\n        binding effects for each transcription factor for a specific gene\n    :type input_dim: int\n    :param output_dim: The number of output features of our model, this is the\n        predicted gene expression value for each TF\n    :type output_dim: int\n    :param lr: The learning rate for the optimizer\n    :type lr: float\n    :raises TypeError: If input_dim is not an integer\n    :raises TypeError: If output_dim is not an integer\n    :raises TypeError: If lr is not a positive float\n    :raises ValueError: If input_dim or output_dim are not positive\n\n    \"\"\"\n    if not isinstance(input_dim, int):\n        raise TypeError(\"input_dim must be an integer\")\n    if not isinstance(output_dim, int):\n        raise TypeError(\"output_dim must be an integer\")\n    if not isinstance(lr, float) or lr &lt;= 0:\n        raise TypeError(\"lr must be a positive float\")\n    if input_dim &lt; 1 or output_dim &lt; 1:\n        raise ValueError(\"input_dim and output_dim must be positive integers\")\n\n    super().__init__()\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.lr = lr\n    self.save_hyperparameters()\n\n    self.mae = MeanAbsoluteError()\n    self.SMSE = SMSE()\n\n    # define layers for the model here\n    self.linear1 = nn.Linear(input_dim, output_dim)\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure the optimizer for the model.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>The optimizer for the model</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def configure_optimizers(self) -&gt; Optimizer:\n    \"\"\"\n    Configure the optimizer for the model.\n\n    :return: The optimizer for the model\n    :rtype: Optimizer\n\n    \"\"\"\n    return torch.optim.Adam(self.parameters(), lr=self.lr)\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the model (i.e. how predictions are made for a given input)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input data to the model (minus the target y values)</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The predicted y values for the input x, this is a tensor of shape (batch_size, output_dim)</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the model (i.e. how predictions are made for a given input)\n\n    :param x: The input data to the model (minus the target y values)\n    :type x: torch.Tensor\n    :return: The predicted y values for the input x, this is a tensor of shape\n        (batch_size, output_dim)\n    :rtype: torch.Tensor\n\n    \"\"\"\n    return self.linear1(x)\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Test step for the model, this is called for each batch of data during testing Testing is only performed after training and validation when we have chosen a final model We want to test our final model on unseen data (which is why we use validation sets to \u201ctest\u201d during training)</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to test on (this will have size (batch_size, input_dim)</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the test batch</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def test_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Test step for the model, this is called for each batch of data during testing\n    Testing is only performed after training and validation when we have chosen a\n    final model We want to test our final model on unseen data (which is why we use\n    validation sets to \"test\" during training)\n\n    :param batch: The batch of data to test on (this will have size (batch_size,\n        input_dim)\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the test batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    loss = nn.functional.mse_loss(y_pred, y)\n    self.log(\"test_mse\", loss)\n    self.log(\"test_mae\", self.mae(y_pred, y))\n    self.log(\"test_smse\", self.SMSE(y_pred, y))\n    return loss\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step for the model, this is called for each batch of data during training.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to train on</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the training batch</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Training step for the model, this is called for each batch of data during\n    training.\n\n    :param batch: The batch of data to train on\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the training batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    loss = nn.functional.mse_loss(y_pred, y)\n    self.log(\"train_mse\", loss)\n    self.log(\"train_mae\", self.mae(y_pred, y))\n    self.log(\"train_smse\", self.SMSE(y_pred, y))\n    return loss\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step for the model, this is called for each batch of data during validation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to validate on</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the validation batch</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def validation_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Validation step for the model, this is called for each batch of data during\n    validation.\n\n    :param batch: The batch of data to validate on\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the validation batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    loss = nn.functional.mse_loss(y_pred, y)\n\n    self.log(\"val_mse\", loss)\n    self.log(\"val_mae\", self.mae(y_pred, y))\n    self.log(\"val_smse\", self.SMSE(y_pred, y))\n    return loss\n</code></pre>"},{"location":"probability_models/GenePopulation/","title":"GenePopulation","text":"<p>A simple class to hold a tensor boolean 1D vector where 0 is meant to identify genes which are unaffected by a given TF and 1 is meant to identify genes which are affected by a given TF.</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>class GenePopulation:\n    \"\"\"A simple class to hold a tensor boolean 1D vector where 0 is meant to identify\n    genes which are unaffected by a given TF and 1 is meant to identify genes which are\n    affected by a given TF.\"\"\"\n\n    def __init__(self, labels: torch.Tensor) -&gt; None:\n        \"\"\"\n        Constructor of GenePopulation.\n\n        :param labels: This can be any 1D tensor of boolean values. But it is meant to\n            be the output of `generate_gene_population()`\n        :type labels: torch.Tensor\n        :raises TypeError: If labels is not a tensor\n        :raises ValueError: If labels is not a 1D tensor\n        :raises TypeError: If labels is not a boolean tensor\n\n        \"\"\"\n        if not isinstance(labels, torch.Tensor):\n            raise TypeError(\"labels must be a tensor\")\n        if not labels.ndim == 1:\n            raise ValueError(\"labels must be a 1D tensor\")\n        if not labels.dtype == torch.bool:\n            raise TypeError(\"labels must be a boolean tensor\")\n        self.labels = labels\n\n    def __repr__(self):\n        return f\"&lt;GenePopulation size={len(self.labels)}&gt;\"\n</code></pre>"},{"location":"probability_models/GenePopulation/#yeastdnnexplorer.probability_models.generate_data.GenePopulation.__init__","title":"<code>__init__(labels)</code>","text":"<p>Constructor of GenePopulation.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Tensor</code> <p>This can be any 1D tensor of boolean values. But it is meant to be the output of <code>generate_gene_population()</code></p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If labels is not a tensor</p> <code>ValueError</code> <p>If labels is not a 1D tensor</p> <code>TypeError</code> <p>If labels is not a boolean tensor</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def __init__(self, labels: torch.Tensor) -&gt; None:\n    \"\"\"\n    Constructor of GenePopulation.\n\n    :param labels: This can be any 1D tensor of boolean values. But it is meant to\n        be the output of `generate_gene_population()`\n    :type labels: torch.Tensor\n    :raises TypeError: If labels is not a tensor\n    :raises ValueError: If labels is not a 1D tensor\n    :raises TypeError: If labels is not a boolean tensor\n\n    \"\"\"\n    if not isinstance(labels, torch.Tensor):\n        raise TypeError(\"labels must be a tensor\")\n    if not labels.ndim == 1:\n        raise ValueError(\"labels must be a 1D tensor\")\n    if not labels.dtype == torch.bool:\n        raise TypeError(\"labels must be a boolean tensor\")\n    self.labels = labels\n</code></pre>"},{"location":"probability_models/default_perturbation_effect_adjustment_function/","title":"Probability Models","text":"<p>Default function to adjust the mean of the perturbation effect based on the enrichment score.</p> <p>All functions that are passed to generate_perturbation_effects() in the argument adjustment_function must have the same signature as this function.</p> <p>Parameters:</p> Name Type Description Default <code>binding_enrichment_data</code> <code>Tensor</code> <p>A tensor of enrichment scores for each gene with dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a matrix with columns [label, enrichment, pvalue].</p> required <code>signal_mean</code> <code>float</code> <p>The mean for signal genes.</p> required <code>noise_mean</code> <code>float</code> <p>The mean for noise genes.</p> required <code>max_adjustment</code> <code>float</code> <p>The maximum adjustment to the base mean based on enrichment.</p> required <code>tf_relationships</code> <code>dict[int, list[int]], optional</code> <p>Unused in this function. It is only here to match the signature of the other adjustment functions.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Adjusted mean as a tensor.</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def default_perturbation_effect_adjustment_function(\n    binding_enrichment_data: torch.Tensor,\n    signal_mean: float,\n    noise_mean: float,\n    max_adjustment: float,\n    **kwargs,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Default function to adjust the mean of the perturbation effect based on the\n    enrichment score.\n\n    All functions that are passed to generate_perturbation_effects() in the argument\n    adjustment_function must have the same signature as this function.\n\n    :param binding_enrichment_data: A tensor of enrichment scores for each gene with\n        dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a\n        matrix with columns [label, enrichment, pvalue].\n    :type binding_enrichment_data: torch.Tensor\n    :param signal_mean: The mean for signal genes.\n    :type signal_mean: float\n    :param noise_mean: The mean for noise genes.\n    :type noise_mean: float\n    :param max_adjustment: The maximum adjustment to the base mean based on enrichment.\n    :type max_adjustment: float\n    :param tf_relationships: Unused in this function. It is only here to match the\n        signature of the other adjustment functions.\n    :type tf_relationships: dict[int, list[int]], optional\n    :return: Adjusted mean as a tensor.\n    :rtype: torch.Tensor\n\n    \"\"\"\n    # Extract signal/noise labels and enrichment scores\n    signal_labels = binding_enrichment_data[:, :, 0]\n    enrichment_scores = binding_enrichment_data[:, :, 1]\n\n    adjusted_mean_matrix = torch.where(\n        signal_labels == 1, enrichment_scores, torch.zeros_like(enrichment_scores)\n    )\n\n    for gene_idx in range(signal_labels.shape[0]):\n        for tf_index in range(signal_labels.shape[1]):\n            if signal_labels[gene_idx, tf_index] == 1:\n                # draw a random value between 0 and 1 to use to control\n                # magnitude of adjustment\n                adjustment_multiplier = torch.rand(1)\n\n                # randomly adjust the gene by some portion of the max adjustment\n                adjusted_mean_matrix[gene_idx, tf_index] = signal_mean + (\n                    adjustment_multiplier * max_adjustment\n                )\n            else:\n                # related tfs are not all bound, so set the enrichment\n                # score to noise mean\n                adjusted_mean_matrix[gene_idx, tf_index] = noise_mean\n\n    return adjusted_mean_matrix\n</code></pre>"},{"location":"probability_models/generate_binding_effects/","title":"Generate binding effects","text":"<p>Generate enrichment effects for genes using vectorized operations, based on their signal designation, with separate experiment hops ranges for noise and signal genes.</p> <p>Note that the default values are a scaled down version of actual data. See also https://github.com/cmatKhan/callingCardsTools/blob/main/callingcardstools/PeakCalling/yeast/enrichment.py</p> <p>Parameters:</p> Name Type Description Default <code>gene_population</code> <code>GenePopulation</code> <p>A GenePopulation object. See <code>generate_gene_population()</code></p> required <code>background_hops_range</code> <code>tuple[int, int]</code> <p>The range of hops for background genes. Defaults to (1, 100)</p> <code>(1, 100)</code> <code>noise_experiment_hops_range</code> <code>tuple[int, int]</code> <p>The range of hops for noise genes. Defaults to (0, 1)</p> <code>(0, 1)</code> <code>signal_experiment_hops_range</code> <code>tuple[int, int]</code> <p>The range of hops for signal genes. Defaults to (1, 6)</p> <code>(1, 6)</code> <code>total_background_hops</code> <code>int</code> <p>The total number of background hops. Defaults to 1000</p> <code>1000</code> <code>total_experiment_hops</code> <code>int</code> <p>The total number of experiment hops. Defaults to 76</p> <code>76</code> <code>pseudocount</code> <code>float</code> <p>A pseudocount to avoid division by zero. Defaults to 1e-10</p> <code>1e-10</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>A tensor of enrichment values for each gene.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If gene_population is not a GenePopulation object</p> <code>TypeError</code> <p>If total_background_hops is not an integer</p> <code>TypeError</code> <p>If total_experiment_hops is not an integer</p> <code>TypeError</code> <p>If pseudocount is not a float</p> <code>TypeError</code> <p>If background_hops_range is not a tuple</p> <code>TypeError</code> <p>If noise_experiment_hops_range is not a tuple</p> <code>TypeError</code> <p>If signal_experiment_hops_range is not a tuple</p> <code>ValueError</code> <p>If background_hops_range is not a tuple of length 2</p> <code>ValueError</code> <p>If noise_experiment_hops_range is not a tuple of length 2</p> <code>ValueError</code> <p>If signal_experiment_hops_range is not a tuple of length 2</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def generate_binding_effects(\n    gene_population: GenePopulation,\n    background_hops_range: tuple[int, int] = (1, 100),\n    noise_experiment_hops_range: tuple[int, int] = (0, 1),\n    signal_experiment_hops_range: tuple[int, int] = (1, 6),\n    total_background_hops: int = 1000,\n    total_experiment_hops: int = 76,\n    pseudocount: float = 1e-10,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Generate enrichment effects for genes using vectorized operations, based on their\n    signal designation, with separate experiment hops ranges for noise and signal genes.\n\n    Note that the default values are a scaled down version of actual data. See also\n    https://github.com/cmatKhan/callingCardsTools/blob/main/callingcardstools/PeakCalling/yeast/enrichment.py\n\n    :param gene_population: A GenePopulation object. See `generate_gene_population()`\n    :type gene_population: GenePopulation\n    :param background_hops_range: The range of hops for background genes. Defaults to\n        (1, 100)\n    :type background_hops_range: Tuple[int, int], optional\n    :param noise_experiment_hops_range: The range of hops for noise genes. Defaults to\n        (0, 1)\n    :type noise_experiment_hops_range: Tuple[int, int], optional\n    :param signal_experiment_hops_range: The range of hops for signal genes. Defaults to\n        (1, 6)\n    :type signal_experiment_hops_range: Tuple[int, int], optional\n    :param total_background_hops: The total number of background hops. Defaults to 1000\n    :type total_background_hops: int, optional\n    :param total_experiment_hops: The total number of experiment hops. Defaults to 76\n    :type total_experiment_hops: int, optional\n    :param pseudocount: A pseudocount to avoid division by zero. Defaults to 1e-10\n    :type pseudocount: float, optional\n    :return: A tensor of enrichment values for each gene.\n    :rtype: torch.Tensor\n    :raises TypeError: If gene_population is not a GenePopulation object\n    :raises TypeError: If total_background_hops is not an integer\n    :raises TypeError: If total_experiment_hops is not an integer\n    :raises TypeError: If pseudocount is not a float\n    :raises TypeError: If background_hops_range is not a tuple\n    :raises TypeError: If noise_experiment_hops_range is not a tuple\n    :raises TypeError: If signal_experiment_hops_range is not a tuple\n    :raises ValueError: If background_hops_range is not a tuple of length 2\n    :raises ValueError: If noise_experiment_hops_range is not a tuple of length 2\n    :raises ValueError: If signal_experiment_hops_range is not a tuple of length 2\n\n    \"\"\"\n    # NOTE: torch intervals are half open on the right, so we add 1 to the\n    # high end of the range to make it inclusive\n\n    # check input\n    if not isinstance(gene_population, GenePopulation):\n        raise TypeError(\"gene_population must be a GenePopulation object\")\n    if not isinstance(total_background_hops, int):\n        raise TypeError(\"total_background_hops must be an integer\")\n    if not isinstance(total_experiment_hops, int):\n        raise TypeError(\"total_experiment_hops must be an integer\")\n    if not isinstance(pseudocount, float):\n        raise TypeError(\"pseudocount must be a float\")\n    for arg, tup in {\n        \"background_hops_range\": background_hops_range,\n        \"noise_experiment_hops_range\": noise_experiment_hops_range,\n        \"signal_experiment_hops_range\": signal_experiment_hops_range,\n    }.items():\n        if not isinstance(tup, tuple):\n            raise TypeError(f\"{arg} must be a tuple\")\n        if not len(tup) == 2:\n            raise ValueError(f\"{arg} must be a tuple of length 2\")\n        if not all(isinstance(i, int) for i in tup):\n            raise TypeError(f\"{arg} must be a tuple of integers\")\n\n    # Generate background hops for all genes\n    background_hops = torch.randint(\n        low=background_hops_range[0],\n        high=background_hops_range[1] + 1,\n        size=(gene_population.labels.shape[0],),\n    )\n\n    # Generate experiment hops noise genes\n    noise_experiment_hops = torch.randint(\n        low=noise_experiment_hops_range[0],\n        high=noise_experiment_hops_range[1] + 1,\n        size=(gene_population.labels.shape[0],),\n    )\n    # Generate experiment hops signal genes\n    signal_experiment_hops = torch.randint(\n        low=signal_experiment_hops_range[0],\n        high=signal_experiment_hops_range[1] + 1,\n        size=(gene_population.labels.shape[0],),\n    )\n\n    # Use signal designation to select appropriate experiment hops\n    experiment_hops = torch.where(\n        gene_population.labels == 1, signal_experiment_hops, noise_experiment_hops\n    )\n\n    # Calculate enrichment for all genes\n    return (experiment_hops.float() / (total_experiment_hops + pseudocount)) / (\n        (background_hops.float() / (total_background_hops + pseudocount)) + pseudocount\n    )\n</code></pre>"},{"location":"probability_models/generate_gene_population/","title":"Generate Gene Population","text":"<p>Generate two sets of genes, one of which will be considered genes which show a signal, and the other which does not. The return is a one dimensional boolean tensor where a value of \u20180\u2019 means that the gene at that index is part of the noise group and a \u20181\u2019 means the gene at that index is part of the signal group. The length of the tensor is the number of genes in this simulated organism.</p> <p>Parameters:</p> Name Type Description Default <code>total</code> <code>int</code> <p>The total number of genes. defaults to 1000</p> <code>1000</code> <code>signal_group</code> <code>float</code> <p>The proportion of genes in the signal group. defaults to 0.3</p> <code>0.3</code> <p>Returns:</p> Type Description <code>GenePopulation</code> <p>A one dimensional tensor of boolean values where the set of indices with a value of \u20181\u2019 are the signal group and the set of indices with a value of \u20180\u2019 are the noise group.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>if total is not an integer</p> <code>ValueError</code> <p>If signal_group is not between 0 and 1</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def generate_gene_population(\n    total: int = 1000, signal_group: float = 0.3\n) -&gt; GenePopulation:\n    \"\"\"\n    Generate two sets of genes, one of which will be considered genes which show a\n    signal, and the other which does not. The return is a one dimensional boolean tensor\n    where a value of '0' means that the gene at that index is part of the noise group\n    and a '1' means the gene at that index is part of the signal group. The length of\n    the tensor is the number of genes in this simulated organism.\n\n    :param total: The total number of genes. defaults to 1000\n    :type total: int, optional\n    :param signal_group: The proportion of genes in the signal group. defaults to 0.3\n    :type signal_group: float, optional\n    :return: A one dimensional tensor of boolean values where the set of indices with a\n        value of '1' are the signal group and the set of indices with a value of '0' are\n        the noise group.\n    :rtype: GenePopulation\n    :raises TypeError: if total is not an integer\n    :raises ValueError: If signal_group is not between 0 and 1\n\n    \"\"\"\n    if not isinstance(total, int):\n        raise TypeError(\"total must be an integer\")\n    if not 0 &lt;= signal_group &lt;= 1:\n        raise ValueError(\"signal_group must be between 0 and 1\")\n\n    signal_group_size = int(total * signal_group)\n    logger.info(\"Generating %s genes with signal\", signal_group_size)\n\n    labels = torch.cat(\n        (\n            torch.ones(signal_group_size, dtype=torch.bool),\n            torch.zeros(total - signal_group_size, dtype=torch.bool),\n        )\n    )[torch.randperm(total)]\n\n    return GenePopulation(labels)\n</code></pre>"},{"location":"probability_models/generate_perturbation_effects/","title":"Generate perturbation effects","text":"<p>Generate perturbation effects for genes.</p> <p>If <code>max_mean_adjustment</code> is greater than 0, then the mean of the effects are adjusted based on the binding_data and the function passed in <code>adjustment_function</code>. See <code>default_perturbation_effect_adjustment_function()</code> for the default option. If <code>max_mean_adjustment</code> is 0, then the mean is not adjusted. Additional keyword arguments may be passed in that will be passed along to the adjustment function.</p> <p>Parameters:</p> Name Type Description Default <code>binding_data</code> <code>Tensor</code> <p>A tensor of binding data with dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a matrix with columns [label, enrichment, pvalue].</p> required <code>tf_index</code> <code>int | None</code> <p>The index of the TF in the binding_data tensor. Not used if we are adjusting the means (ie only used if max_mean_adjustment == 0). Defaults to None</p> <code>None</code> <code>noise_mean</code> <code>float</code> <p>The mean for noise genes. Defaults to 0.0</p> <code>0.0</code> <code>noise_std</code> <code>float</code> <p>The standard deviation for noise genes. Defaults to 1.0</p> <code>1.0</code> <code>signal_mean</code> <code>float</code> <p>The mean for signal genes. Defaults to 3.0</p> <code>3.0</code> <code>signal_std</code> <code>float</code> <p>The standard deviation for signal genes. Defaults to 1.0</p> <code>1.0</code> <code>max_mean_adjustment</code> <code>float</code> <p>The maximum adjustment to the base mean based on enrichment. Defaults to 0.0</p> <code>0.0</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>A tensor of perturbation effects for each gene.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If binding_data is not a 3D tensor with the third dimension having a length of 3</p> <code>ValueError</code> <p>If noise_mean, noise_std, signal_mean, signal_std, or max_mean_adjustment are not floats</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def generate_perturbation_effects(\n    binding_data: torch.Tensor,\n    tf_index: int | None = None,\n    noise_mean: float = 0.0,\n    noise_std: float = 1.0,\n    signal_mean: float = 3.0,\n    signal_std: float = 1.0,\n    max_mean_adjustment: float = 0.0,\n    adjustment_function: Callable[\n        [torch.Tensor, float, float, float], torch.Tensor\n    ] = default_perturbation_effect_adjustment_function,\n    **kwargs,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Generate perturbation effects for genes.\n\n    If `max_mean_adjustment` is greater than 0, then the mean of the\n    effects are adjusted based on the binding_data and the function passed\n    in `adjustment_function`. See `default_perturbation_effect_adjustment_function()`\n    for the default option. If `max_mean_adjustment` is 0, then the mean\n    is not adjusted. Additional keyword arguments may be passed in that will be\n    passed along to the adjustment function.\n\n    :param binding_data: A tensor of binding data with dimensions [n_genes, n_tfs, 3]\n        where the entries in the third dimension are a matrix with columns\n        [label, enrichment, pvalue].\n    :type binding_data: torch.Tensor\n    :param tf_index: The index of the TF in the binding_data tensor. Not used if we\n        are adjusting the means (ie only used if max_mean_adjustment == 0).\n        Defaults to None\n    :type tf_index: int\n    :param noise_mean: The mean for noise genes. Defaults to 0.0\n    :type noise_mean: float, optional\n    :param noise_std: The standard deviation for noise genes. Defaults to 1.0\n    :type noise_std: float, optional\n    :param signal_mean: The mean for signal genes. Defaults to 3.0\n    :type signal_mean: float, optional\n    :param signal_std: The standard deviation for signal genes. Defaults to 1.0\n    :type signal_std: float, optional\n    :param max_mean_adjustment: The maximum adjustment to the base mean based\n        on enrichment. Defaults to 0.0\n    :type max_mean_adjustment: float, optional\n\n    :return: A tensor of perturbation effects for each gene.\n    :rtype: torch.Tensor\n\n    :raises ValueError: If binding_data is not a 3D tensor with the third\n        dimension having a length of 3\n    :raises ValueError: If noise_mean, noise_std, signal_mean, signal_std,\n        or max_mean_adjustment are not floats\n\n    \"\"\"\n    # check that a valid combination of inputs has been passed in\n    if max_mean_adjustment == 0.0 and tf_index is None:\n        raise ValueError(\"If max_mean_adjustment is 0, then tf_index must be specified\")\n\n    if binding_data.ndim != 3 or binding_data.shape[2] != 3:\n        raise ValueError(\n            \"enrichment_tensor must have dimensions [num_genes, num_TFs, \"\n            \"[label, enrichment, pvalue]]\"\n        )\n    # check the rest of the inputs\n    if not all(\n        isinstance(i, float)\n        for i in (noise_mean, noise_std, signal_mean, signal_std, max_mean_adjustment)\n    ):\n        raise ValueError(\n            \"noise_mean, noise_std, signal_mean, signal_std, \"\n            \"and max_mean_adjustment must be floats\"\n        )\n    # check the Callable signature\n    if not all(\n        i in inspect.signature(adjustment_function).parameters\n        for i in (\n            \"binding_enrichment_data\",\n            \"signal_mean\",\n            \"noise_mean\",\n            \"max_adjustment\",\n        )\n    ):\n        raise ValueError(\n            \"adjustment_function must have the signature \"\n            \"(binding_enrichment_data, signal_mean, noise_mean, max_adjustment)\"\n        )\n\n    # Initialize an effects tensor for all genes\n    effects = torch.empty(\n        binding_data.size(0), dtype=torch.float32, device=binding_data.device\n    )\n\n    # Randomly assign signs for each gene\n    # fmt: off\n    signs = torch.randint(0, 2, (effects.size(0),),\n                          dtype=torch.float32,\n                          device=binding_data.device) * 2 - 1\n    # fmt: on\n\n    # Apply adjustments to the base mean for the signal genes, if necessary\n    if max_mean_adjustment &gt; 0 and adjustment_function is not None:\n        # Assuming adjustment_function returns a vector of means for each gene.\n        # Signal genes that meet the criteria for adjustment will be affected by\n        # the status of the TFs. What TFs affect a given gene must be specified by\n        # the adjustment_function()\n        adjusted_means = adjustment_function(\n            binding_data,\n            signal_mean,\n            noise_mean,\n            max_mean_adjustment,\n            **kwargs,\n        )\n\n        # add adjustments, ensuring they respect the original sign\n        if adjusted_means.ndim == 1:\n            effects = signs * torch.abs(\n                torch.normal(mean=adjusted_means, std=signal_std)\n            )\n        else:\n            effects = torch.zeros_like(adjusted_means)\n            for col_idx in range(effects.size(1)):\n                effects[:, col_idx] = signs * torch.abs(\n                    torch.normal(mean=adjusted_means[:, col_idx], std=signal_std)\n                )\n    else:\n        signal_mask = binding_data[:, tf_index, 0] == 1\n\n        # Generate effects based on the noise and signal means, applying the sign\n        effects[~signal_mask] = signs[~signal_mask] * torch.abs(\n            torch.normal(\n                mean=noise_mean, std=noise_std, size=(torch.sum(~signal_mask),)\n            )\n        )\n        effects[signal_mask] = signs[signal_mask] * torch.abs(\n            torch.normal(\n                mean=signal_mean, std=signal_std, size=(torch.sum(signal_mask),)\n            )\n        )\n\n    return effects\n</code></pre>"},{"location":"probability_models/generate_pvalues/","title":"Generate pvalues","text":"<p>Generate p-values for genes where larger effects are less likely to be false positives.</p> <p>Parameters:</p> Name Type Description Default <code>effects</code> <code>Tensor</code> <p>A tensor of effects</p> required <code>large_effect_percentile</code> <code>float</code> <p>The percentile of effects that are considered large effects. Defaults to 0.9</p> <code>0.9</code> <code>large_effect_upper_pval</code> <code>float</code> <p>The upper bound of the p-values for large effects. Defaults to 0.2</p> <code>0.2</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>A tensor of p-values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If effects is not a tensor or the values themselves are not numeric</p> <code>ValueError</code> <p>If large_effect_percentile is not between 0 and 1</p> <code>ValueError</code> <p>If large_effect_upper_pval is not between 0 and 1</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def generate_pvalues(\n    effects: torch.Tensor,\n    large_effect_percentile: float = 0.9,\n    large_effect_upper_pval: float = 0.2,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Generate p-values for genes where larger effects are less likely to be false\n    positives.\n\n    :param effects: A tensor of effects\n    :type effects: torch.Tensor\n    :param large_effect_percentile: The percentile of effects that are considered large\n        effects. Defaults to 0.9\n    :type large_effect_percentile: float, optional\n    :param large_effect_upper_pval: The upper bound of the p-values for large effects.\n        Defaults to 0.2\n    :return: A tensor of p-values\n    :rtype: torch.Tensor\n    :raises ValueError: If effects is not a tensor or the values themselves are not\n        numeric\n    :raises ValueError: If large_effect_percentile is not between 0 and 1\n    :raises ValueError: If large_effect_upper_pval is not between 0 and 1\n\n    \"\"\"\n    # check inputs\n    if not isinstance(effects, torch.Tensor):\n        raise ValueError(\"effects must be a tensor\")\n    if not torch.is_floating_point(effects):\n        raise ValueError(\"effects must be numeric\")\n    if not 0 &lt;= large_effect_percentile &lt;= 1:\n        raise ValueError(\"large_effect_percentile must be between 0 and 1\")\n    if not 0 &lt;= large_effect_upper_pval &lt;= 1:\n        raise ValueError(\"large_effect_upper_pval must be between 0 and 1\")\n\n    # Generate p-values\n    pvalues = torch.rand(effects.shape[0])\n\n    # Draw p-values from a uniform distribution where larger abs(effects) are\n    # less likely to be false positives\n    large_effect_threshold = torch.quantile(torch.abs(effects), large_effect_percentile)\n    large_effect_mask = torch.abs(effects) &gt;= large_effect_threshold\n    pvalues[large_effect_mask] = (\n        torch.rand(torch.sum(large_effect_mask)) * large_effect_upper_pval\n    )\n\n    return pvalues\n</code></pre>"},{"location":"probability_models/perturbation_effect_adjustment_function_with_tf_relationships/","title":"Perturbation effect adjustment function with tf relationships","text":"<p>Adjust the mean of the perturbation effect based on the enrichment score and the provided relationships between TFs. For each gene, the mean of the TF-gene pair\u2019s perturbation effect will be adjusted if the TF is bound to the gene and all related TFs are also bound to the gene. The adjustment will be a random value not exceeding the maximum adjustment.</p> <p>Parameters:</p> Name Type Description Default <code>binding_enrichment_data</code> <code>Tensor</code> <p>A tensor of enrichment scores for each gene with dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a matrix with columns [label, enrichment, pvalue].</p> required <code>signal_mean</code> <code>float</code> <p>The mean for signal genes.</p> required <code>noise_mean</code> <code>float</code> <p>The mean for noise genes.</p> required <code>max_adjustment</code> <code>float</code> <p>The maximum adjustment to the base mean based on enrichment.</p> required <code>tf_relationships</code> <code>dict[int, list[int]]</code> <p>A dictionary where the keys are the indices of the TFs and the values are lists of indices of other TFs that are related to the key TF.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Adjusted mean as a tensor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tf_relationships is not a dictionary between ints and lists of ints</p> <code>ValueError</code> <p>If the tf_relationships dict does not have the same number of TFs as the binding_data tensor passed into the function</p> <code>ValueError</code> <p>If the tf_relationships dict has any TFs in the values that are not also in the keys or any key or value TFs that are out of bounds for the binding_data tensor</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def perturbation_effect_adjustment_function_with_tf_relationships(\n    binding_enrichment_data: torch.Tensor,\n    signal_mean: float,\n    noise_mean: float,\n    max_adjustment: float,\n    tf_relationships: dict[int, list[int]],\n) -&gt; torch.Tensor:\n    \"\"\"\n    Adjust the mean of the perturbation effect based on the enrichment score and the\n    provided relationships between TFs. For each gene, the mean of the TF-gene pair's\n    perturbation effect will be adjusted if the TF is bound to the gene and all related\n    TFs are also bound to the gene. The adjustment will be a random value not exceeding\n    the maximum adjustment.\n\n    :param binding_enrichment_data: A tensor of enrichment scores for each gene with\n        dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a\n        matrix with columns [label, enrichment, pvalue].\n    :type binding_enrichment_data: torch.Tensor\n    :param signal_mean: The mean for signal genes.\n    :type signal_mean: float\n    :param noise_mean: The mean for noise genes.\n    :type noise_mean: float\n    :param max_adjustment: The maximum adjustment to the base mean based on enrichment.\n    :type max_adjustment: float\n    :param tf_relationships: A dictionary where the keys are the indices of the TFs and\n        the values are lists of indices of other TFs that are related to the key TF.\n    :type tf_relationships: dict[int, list[int]]\n    :return: Adjusted mean as a tensor.\n    :rtype: torch.Tensor\n    :raises ValueError: If tf_relationships is not a dictionary between ints and lists\n        of ints\n    :raises ValueError: If the tf_relationships dict does not have the same number of\n        TFs as the binding_data tensor passed into the function\n    :raises ValueError: If the tf_relationships dict has any TFs in the values that are\n        not also in the keys or any key or value TFs that are out of bounds for the\n        binding_data tensor\n\n    \"\"\"\n    if (\n        not isinstance(tf_relationships, dict)\n        or not all(isinstance(v, list) for v in tf_relationships.values())\n        or not all(isinstance(k, int) for k in tf_relationships.keys())\n        or not all(isinstance(i, int) for v in tf_relationships.values() for i in v)\n    ):\n        raise ValueError(\n            \"tf_relationships must be a dictionary between ints and lists of ints\"\n        )\n    if not all(\n        k in range(binding_enrichment_data.shape[1]) for k in tf_relationships.keys()\n    ) or not all(\n        i in range(binding_enrichment_data.shape[1])\n        for v in tf_relationships.values()\n        for i in v\n    ):\n        raise ValueError(\n            \"all keys and values in tf_relationships must be within the \\\n                  bounds of the binding_data tensor's number of TFs\"\n        )\n    if not len(tf_relationships) == binding_enrichment_data.shape[1]:\n        raise ValueError(\n            \"tf_relationships must have the same number of TFs as the \\\n                binding_data tensor passed into the function\"\n        )\n\n    # Extract signal/noise labels and enrichment scores\n    signal_labels = binding_enrichment_data[:, :, 0]  # shape: (num_genes, num_tfs)\n    enrichment_scores = binding_enrichment_data[:, :, 1]  # shape: (num_genes, num_tfs)\n\n    # we set all unbound scores to 0, then we will go through and also\n    # set any bound scores to noise_mean if the related tfs are not also bound\n    adjusted_mean_matrix = torch.where(\n        signal_labels == 1, enrichment_scores, torch.zeros_like(enrichment_scores)\n    )  # shape: (num_genes, num_tfs)\n\n    for gene_idx in range(signal_labels.shape[0]):\n        for tf_index, related_tfs in tf_relationships.items():\n            if signal_labels[gene_idx, tf_index] == 1 and torch.all(\n                signal_labels[gene_idx, related_tfs] == 1\n            ):\n                # draw a random value between 0 and 1 to use to\n                # control magnitude of adjustment\n                adjustment_multiplier = torch.rand(1)\n\n                # randomly adjust the gene by some portion of the max adjustment\n                adjusted_mean_matrix[gene_idx, tf_index] = signal_mean + (\n                    adjustment_multiplier * max_adjustment\n                )\n            else:\n                # related tfs are not all bound, set the enrichment score to noise mean\n                adjusted_mean_matrix[gene_idx, tf_index] = noise_mean\n\n    return adjusted_mean_matrix  # shape (num_genes, num_tfs)\n</code></pre>"},{"location":"probability_models/perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic/","title":"Perturbation effect adjustment function with tf relationships boolean logic","text":"<p>Adjust the mean of the perturbation effect based on the enrichment score and the provided binary / boolean or unary relationships between TFs. For each gene, the mean of the TF-gene pair\u2019s perturbation effect will be adjusted if the TF is bound to the gene and all of the Relations associated with the TF are satisfied (ie they evaluate to True). These relations could be unary conditions or Ands or Ors between TFs. A TF being bound corresponds to a true value, which means And(4, 5) would be satisfied is both TF 4 and TF 5 are bound to the gene in question. The adjustment will be a random value not exceeding the maximum adjustment.</p> <p>Parameters:</p> Name Type Description Default <code>binding_enrichment_data</code> <code>Tensor</code> <p>A tensor of enrichment scores for each gene with dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a matrix with columns [label, enrichment, pvalue].</p> required <code>signal_mean</code> <code>float</code> <p>The mean for signal genes.</p> required <code>noise_mean</code> <code>float</code> <p>The mean for noise genes.</p> required <code>max_adjustment</code> <code>float</code> <p>The maximum adjustment to the base mean based on enrichment.</p> required <code>tf_relationships</code> <code>dict[int, list[Relation]]</code> <p>A dictionary where the keys are TF indices and the values are lists of Relation objects that represent the conditions that must be met for the mean of the perturbation effect associated with the TF-gene pair to be adjusted.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Adjusted mean as a tensor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tf_relationships is not a dictionary between ints and lists of Relations</p> <code>ValueError</code> <p>If the tf_relationships dict does not have the same number of TFs as the binding_data tensor passed into the function</p> <code>ValueError</code> <p>If the tf_relationships dict has any TFs in the values that are not also in the keys or any key or value TFs that are out of bounds for the binding_data tensor</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic(\n    binding_enrichment_data: torch.Tensor,\n    signal_mean: float,\n    noise_mean: float,\n    max_adjustment: float,\n    tf_relationships: dict[int, list[Relation]],\n) -&gt; torch.Tensor:\n    \"\"\"\n    Adjust the mean of the perturbation effect based on the enrichment score and the\n    provided binary / boolean or unary relationships between TFs. For each gene, the\n    mean of the TF-gene pair's perturbation effect will be adjusted if the TF is bound\n    to the gene and all of the Relations associated with the TF are satisfied (ie they\n    evaluate to True). These relations could be unary conditions or Ands or Ors between\n    TFs. A TF being bound corresponds to a true value, which means And(4, 5) would be\n    satisfied is both TF 4 and TF 5 are bound to the gene in question. The adjustment\n    will be a random value not exceeding the maximum adjustment.\n\n    :param binding_enrichment_data: A tensor of enrichment scores for each gene with\n        dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a\n        matrix with columns [label, enrichment, pvalue].\n    :type binding_enrichment_data: torch.Tensor\n    :param signal_mean: The mean for signal genes.\n    :type signal_mean: float\n    :param noise_mean: The mean for noise genes.\n    :type noise_mean: float\n    :param max_adjustment: The maximum adjustment to the base mean based on enrichment.\n    :type max_adjustment: float\n    :param tf_relationships: A dictionary where the keys are TF indices and the values\n        are lists of Relation objects that represent the conditions that must be met for\n        the mean of the perturbation effect associated with the TF-gene pair to be\n        adjusted.\n    :type tf_relationships: dict[int, list[Relation]]\n    :return: Adjusted mean as a tensor.\n    :rtype: torch.Tensor\n    :raises ValueError: If tf_relationships is not a dictionary between ints and lists\n        of Relations\n    :raises ValueError: If the tf_relationships dict does not have the same number of\n        TFs as the binding_data tensor passed into the function\n    :raises ValueError: If the tf_relationships dict has any TFs in the values that are\n        not also in the keys or any key or value TFs that are out of bounds for the\n        binding_data tensor\n\n    \"\"\"\n    if (\n        not isinstance(tf_relationships, dict)\n        or not all(isinstance(v, list) for v in tf_relationships.values())\n        or not all(isinstance(k, int) for k in tf_relationships.keys())\n        or not all(\n            isinstance(i, Relation) for v in tf_relationships.values() for i in v\n        )\n    ):\n        raise ValueError(\n            \"tf_relationships must be a dictionary between \\\n                ints and lists of Relation objects\"\n        )\n    if not all(\n        k in range(binding_enrichment_data.shape[1]) for k in tf_relationships.keys()\n    ):\n        raise ValueError(\n            \"all TFs mentioned in tf_relationships must be within \\\n                the bounds of the binding_data tensor's number of TFs\"\n        )\n    if not len(tf_relationships) == binding_enrichment_data.shape[1]:\n        raise ValueError(\n            \"tf_relationships must have the same number of TFs as \\\n                the binding_data tensor passed into the function\"\n        )\n\n    # Extract signal/noise labels and enrichment scores\n    signal_labels = binding_enrichment_data[:, :, 0]  # shape: (num_genes, num_tfs)\n    enrichment_scores = binding_enrichment_data[:, :, 1]  # shape: (num_genes, num_tfs)\n\n    # we set all unbound scores to 0, then we will go through and also set any\n    # bound scores to noise_mean if the related boolean statements are not satisfied\n    adjusted_mean_matrix = torch.where(\n        signal_labels == 1, enrichment_scores, torch.zeros_like(enrichment_scores)\n    )  # shape: (num_genes, num_tfs)\n\n    for gene_idx in range(signal_labels.shape[0]):\n        for tf_index, relations in tf_relationships.items():\n            # check if all relations (boolean relationships)\n            # associated with TFs are satisfied\n            if signal_labels[gene_idx, tf_index] == 1 and all(\n                relation.evaluate(signal_labels[gene_idx].tolist())\n                for relation in relations\n            ):\n                # draw a random value between 0 and 1 to use to\n                # control magnitude of adjustment\n                adjustment_multiplier = torch.rand(1)\n\n                # randomly adjust the gene by some portion of the max adjustment\n                adjusted_mean_matrix[gene_idx, tf_index] = signal_mean + (\n                    adjustment_multiplier * max_adjustment\n                )\n            else:\n                # related tfs are not all bound, set the enrichment score to noise mean\n                adjusted_mean_matrix[gene_idx, tf_index] = noise_mean\n\n    return adjusted_mean_matrix  # shape (num_genes, num_tfs)\n</code></pre>"},{"location":"probability_models/relation_classes/","title":"Relation classes","text":""},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.And","title":"<code>And</code>","text":"<p>             Bases: <code>Relation</code></p> <p>Class for representing the logical AND of multiple conditions Allows nesed conditions, i.e. And(1, Or(2, 3))</p> Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>class And(Relation):\n    \"\"\"Class for representing the logical AND of multiple conditions Allows nesed\n    conditions, i.e. And(1, Or(2, 3))\"\"\"\n\n    def __init__(self, *conditions):\n        \"\"\"\n        :param conditions: List of conditions to be evaluated\n        :type conditions: List[float | Relation]\n        \"\"\"\n        self.conditions = conditions\n\n    def evaluate(self, bound_vec):\n        \"\"\"\n        Returns true if the And() condition evaluates to true Evaluates nested\n        conditions as needed.\n\n        :param bound_vec: Vector of TF indices (0 or 1) indicating which TFs are bound\n            for the gene in question\n        :type bound_vec: List[float]\n\n        \"\"\"\n        if type(bound_vec) is not list or not all(\n            isinstance(x, float) for x in bound_vec\n        ):\n            raise ValueError(\"bound_vec must be a list of floats\")\n\n        if not self.conditions:\n            return True\n\n        # Each condition can be an index or another Relation (And/Or)\n        return all(\n            c.evaluate(bound_vec) if isinstance(c, Relation) else bound_vec[c]\n            for c in self.conditions\n        )\n\n    def __str__(self):\n        return f\"AND({', '.join(str(c) for c in self.conditions)})\"\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.And.__init__","title":"<code>__init__(*conditions)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>List[float | Relation]</code> <p>List of conditions to be evaluated</p> <code>()</code> Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>def __init__(self, *conditions):\n    \"\"\"\n    :param conditions: List of conditions to be evaluated\n    :type conditions: List[float | Relation]\n    \"\"\"\n    self.conditions = conditions\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.And.evaluate","title":"<code>evaluate(bound_vec)</code>","text":"<p>Returns true if the And() condition evaluates to true Evaluates nested conditions as needed.</p> <p>Parameters:</p> Name Type Description Default <code>bound_vec</code> <code>List[float]</code> <p>Vector of TF indices (0 or 1) indicating which TFs are bound for the gene in question</p> required Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>def evaluate(self, bound_vec):\n    \"\"\"\n    Returns true if the And() condition evaluates to true Evaluates nested\n    conditions as needed.\n\n    :param bound_vec: Vector of TF indices (0 or 1) indicating which TFs are bound\n        for the gene in question\n    :type bound_vec: List[float]\n\n    \"\"\"\n    if type(bound_vec) is not list or not all(\n        isinstance(x, float) for x in bound_vec\n    ):\n        raise ValueError(\"bound_vec must be a list of floats\")\n\n    if not self.conditions:\n        return True\n\n    # Each condition can be an index or another Relation (And/Or)\n    return all(\n        c.evaluate(bound_vec) if isinstance(c, Relation) else bound_vec[c]\n        for c in self.conditions\n    )\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.Or","title":"<code>Or</code>","text":"<p>             Bases: <code>Relation</code></p> Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>class Or(Relation):\n    def __init__(self, *conditions):\n        \"\"\"\n        :param conditions: List of conditions to be evaluated\n        :type conditions: List[int | Relation]\n        \"\"\"\n        self.conditions = conditions\n\n    def evaluate(self, bound_vec):\n        \"\"\"\n        Returns true if the Or() condition evaluates to true Evaluates nested conditions\n        as needed.\n\n        :param bound_vec: Vector of TF indices (0 or 1) indicating which TFs are bound\n            for the gene in question\n        :type bound_vec: List[int]\n\n        \"\"\"\n        if type(bound_vec) is not list or not all(\n            isinstance(x, float) for x in bound_vec\n        ):\n            raise ValueError(\"bound_vec must be a list of floats\")\n\n        if not self.conditions:\n            return True\n\n        # Each condition can be an index or another Relation (And/Or)\n        return any(\n            c.evaluate(bound_vec) if isinstance(c, Relation) else bound_vec[c]\n            for c in self.conditions\n        )\n\n    def __str__(self):\n        return f\"OR({', '.join(str(c) for c in self.conditions)})\"\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.Or.__init__","title":"<code>__init__(*conditions)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>List[int | Relation]</code> <p>List of conditions to be evaluated</p> <code>()</code> Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>def __init__(self, *conditions):\n    \"\"\"\n    :param conditions: List of conditions to be evaluated\n    :type conditions: List[int | Relation]\n    \"\"\"\n    self.conditions = conditions\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.Or.evaluate","title":"<code>evaluate(bound_vec)</code>","text":"<p>Returns true if the Or() condition evaluates to true Evaluates nested conditions as needed.</p> <p>Parameters:</p> Name Type Description Default <code>bound_vec</code> <code>List[int]</code> <p>Vector of TF indices (0 or 1) indicating which TFs are bound for the gene in question</p> required Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>def evaluate(self, bound_vec):\n    \"\"\"\n    Returns true if the Or() condition evaluates to true Evaluates nested conditions\n    as needed.\n\n    :param bound_vec: Vector of TF indices (0 or 1) indicating which TFs are bound\n        for the gene in question\n    :type bound_vec: List[int]\n\n    \"\"\"\n    if type(bound_vec) is not list or not all(\n        isinstance(x, float) for x in bound_vec\n    ):\n        raise ValueError(\"bound_vec must be a list of floats\")\n\n    if not self.conditions:\n        return True\n\n    # Each condition can be an index or another Relation (And/Or)\n    return any(\n        c.evaluate(bound_vec) if isinstance(c, Relation) else bound_vec[c]\n        for c in self.conditions\n    )\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.Relation","title":"<code>Relation</code>","text":"<p>Base class for relations between TF indices.</p> Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>class Relation:\n    \"\"\"Base class for relations between TF indices.\"\"\"\n\n    def evaluate(self, bound_vec: list[int]):\n        raise NotImplementedError\n</code></pre>"},{"location":"tutorials/generate_in_silico_data/","title":"Generate In-silico Data","text":"<pre><code>from yeastdnnexplorer.probability_models.relation_classes import And, Or\nfrom yeastdnnexplorer.probability_models.generate_data import (generate_gene_population, \n                                                               generate_binding_effects,\n                                                               generate_pvalues,\n                                                               generate_perturbation_effects,\n                                                               perturbation_effect_adjustment_function_with_tf_relationships,\n                                                               perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic)\n\nimport torch\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(42)  # For CPU\ntorch.cuda.manual_seed_all(42)  # For all CUDA devices\n\n</code></pre> <pre><code>n_genes = 1000\nsignal = [0.1, 0.15, 0.2, 0.25, 0.3]\nn_sample = [1, 1, 2, 2, 4]\n\n# this will be a list of length 10 with a GenePopulation object in each element\ngene_populations_list = []\nfor signal_proportion, n_draws in zip(signal, n_sample):\n    for _ in range(n_draws):\n        gene_populations_list.append(generate_gene_population(n_genes, signal_proportion))\n\n</code></pre> <pre><code># Generate binding data for each gene population\nbinding_effect_list = [generate_binding_effects(gene_population)\n                     for gene_population in gene_populations_list]\n\n\n# Calculate p-values for binding data\nbinding_pvalue_list = [generate_pvalues(binding_data) for binding_data in binding_effect_list]\n\nbinding_data_combined = [torch.stack((gene_population.labels, binding_effect, binding_pval), dim=1)\n                         for gene_population, binding_effect, binding_pval\n                         in zip (gene_populations_list, binding_effect_list, binding_pvalue_list)]\n\n# Stack along a new dimension (dim=1) to create a tensor of shape [num_genes, num_TFs, 3]\nbinding_data_tensor = torch.stack(binding_data_combined, dim=1)\n\n# Verify the shape\nprint(\"Shape of the binding data tensor:\", binding_data_tensor.shape)\n\n</code></pre> <pre>\n<code>Shape of the binding data tensor: torch.Size([1000, 10, 3])\n</code>\n</pre> <pre><code># See `generate_perturbation_effects()` in the help or the documentation for more details.\nperturbation_effects_list_no_mean_adjustment = [generate_perturbation_effects(binding_data_tensor[:, tf_index, :].unsqueeze(1), tf_index=0) \n                                                        for tf_index in range(sum(n_sample))]\nperturbation_pvalue_list_no_mean_adjustment = [generate_pvalues(perturbation_effects) for perturbation_effects in perturbation_effects_list_no_mean_adjustment]\n</code></pre> <pre><code># if you want to modify the default mean for bound genes, you can pass in the 'signal_mean' parameter\nperturbation_effects_list_normal_mean_adjustment = generate_perturbation_effects(\n    binding_data_tensor, \n    max_mean_adjustment=10.0\n)\n\n# since the p-value generation function operates on one column at a time, we must iterate over the columns of our perturb effects\n# list and generate p-values for each column\nperturbation_effects_list_normal_mean_adjustment_pvalues = torch.zeros_like(perturbation_effects_list_normal_mean_adjustment)\nfor col_idx in range(perturbation_effects_list_normal_mean_adjustment.shape[1]):\n    col = perturbation_effects_list_normal_mean_adjustment[:, col_idx]\n    col_pvals = generate_pvalues(col)\n    perturbation_effects_list_normal_mean_adjustment_pvalues[:, col_idx] = col_pvals\n</code></pre> <pre><code># define our dictionary of TF relationships\n# For each gene, if TF 0 is bound, then we only adjust its mean if TF 1 is also bound\n# similarly, if TF 7 is bound, we still only adjust its mean if TFs 1 and 4 are bound\ntf_relationships = {\n    0: [1],\n    1: [8],\n    2: [5, 6],\n    3: [4],\n    4: [5],\n    5: [9],\n    6: [4],\n    7: [1, 4],\n    8: [6],\n    9: [4],\n}\n\nperturbation_effects_list_dep_mean_adjustment = generate_perturbation_effects(\n    binding_data_tensor, \n    tf_relationships=tf_relationships,\n    adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships,\n    max_mean_adjustment=10.0,\n)\nperturbation_effects_list_dep_mean_adjustment_pvalues = torch.zeros_like(perturbation_effects_list_dep_mean_adjustment)\nfor col_idx in range(perturbation_effects_list_dep_mean_adjustment.shape[1]):\n    col = perturbation_effects_list_dep_mean_adjustment[:, col_idx]\n    col_pvals = generate_pvalues(col)\n    perturbation_effects_list_dep_mean_adjustment_pvalues[:, col_idx] = col_pvals\n</code></pre> <pre><code># note that Or(1,1) is used to enforce a unary contraint\ntf_relationships_dict_boolean_logic = {\n    0: [And(3, 4, 8), Or(3, 7), Or(1, 1)],\n    1: [And(5, Or(7, 8))],\n    2: [],\n    3: [Or(7, 9), And(6, 7)],\n    4: [And(1, 2)],\n    5: [Or(0, 1, 2, 8, 9)],\n    6: [And(0, Or(1, 2))],\n    7: [Or(2, And(5, 6, 9))],\n    8: [],\n    9: [And(6, And(3, Or(0, 9)))],\n}\n\nperturbation_effects_list_boolean_logic = generate_perturbation_effects(\n    binding_data_tensor, \n    adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic,\n    tf_relationships=tf_relationships_dict_boolean_logic,\n    max_mean_adjustment=10.0,\n)\nperturbation_effects_list_boolean_logic_pvalues = torch.zeros_like(perturbation_effects_list_boolean_logic)\nfor col_idx in range(perturbation_effects_list_boolean_logic.shape[1]):\n    col = perturbation_effects_list_boolean_logic[:, col_idx]\n    col_pvals = generate_pvalues(col)\n    perturbation_effects_list_boolean_logic_pvalues[:, col_idx] = col_pvals\n</code></pre> <pre><code># Convert lists to tensors if they are not already\nperturbation_effects_tensor = torch.stack(perturbation_effects_list_no_mean_adjustment, dim=1)\nperturbation_pvalues_tensor = torch.stack(perturbation_pvalue_list_no_mean_adjustment, dim=1)\n\n# Ensure perturbation data is reshaped to match [n_genes, n_tfs]\n# This step might need adjustment based on the actual shapes of your tensors.\nperturbation_effects_tensor = perturbation_effects_tensor.unsqueeze(-1)  # Adds an extra dimension for concatenation\nperturbation_pvalues_tensor = perturbation_pvalues_tensor.unsqueeze(-1)  # Adds an extra dimension for concatenation\n\n# Concatenate along the last dimension to form a [n_genes, n_tfs, 5] tensor\nfinal_data_tensor = torch.cat((binding_data_tensor, perturbation_effects_tensor, perturbation_pvalues_tensor), dim=2)\n\n# Verify the shape\nprint(\"Shape of the final data tensor:\", final_data_tensor.shape)\n</code></pre> <pre>\n<code>Shape of the final data tensor: torch.Size([1000, 10, 5])\n</code>\n</pre> <p>As an aside, I choose to structure the data this way by looking at the result of strides, which describes how the data is stored in memory:</p> <pre><code>tensor_continuous = torch.empty(100, 1000, 3)\nstrides_continuous = tensor_continuous.stride()\nprint(strides_continuous)\n\n\ntensor_continuous = torch.empty(1000, 100, 3)\nstrides_continuous = tensor_continuous.stride()\nprint(strides_continuous)\n</code></pre> <pre>\n<code>(3000, 3, 1)\n(300, 3, 1)\n</code>\n</pre> <pre><code>tolerance = 1e-5\nare_equal = torch.isclose(\n    torch.sum(final_data_tensor[:, :, 0] == 1, axis=0),\n    torch.tensor([val * n_genes for val, count in zip(signal, n_sample) for _ in range(count)],\n                 dtype=torch.long),\n    atol=tolerance)\n\nprint(f\"signal/nosie ratio is correct: {are_equal.all()}\")\n</code></pre> <pre>\n<code>signal/nosie ratio is correct: True\n</code>\n</pre> <pre><code>labels = final_data_tensor[:, :, 0].flatten()\nnoise_binding = final_data_tensor[:, :, 1].flatten()[labels == 0]\nsignal_binding = final_data_tensor[:, :, 1].flatten()[labels == 1]\n\nprint(f\"The noise binding max is {noise_binding.max()} and the min is {noise_binding.min()}\")\nprint(f\"the noise min is {noise_binding.min()}\")\nprint(f\"the noise mean is {noise_binding.mean()} and the std is {noise_binding.std()}\")\nprint(f\"The signal binding max is {signal_binding.max()} and the min is {signal_binding.min()}\")\nprint(f\"the signal min is {signal_binding.min()}\")\nprint(f\"the signal mean is {signal_binding.mean()} and the std is {signal_binding.std()}\")\n</code></pre> <pre>\n<code>The noise binding max is 13.157892227172852 and the min is 0.0\nthe noise min is 0.0\nthe noise mean is 0.3589712679386139 and the std is 1.1559306383132935\nThe signal binding max is 78.94734954833984 and the min is 0.1315789520740509\nthe signal min is 0.1315789520740509\nthe signal mean is 2.4840002059936523 and the std is 6.374814510345459\n</code>\n</pre> <pre><code>\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.hist(noise_binding, bins=30, alpha=0.5, label='Label 0', color='orange')\nplt.hist(signal_binding, bins=30, alpha=0.5, label='Label 1', color='blue')\nplt.xlim(0,5)\nplt.title('Histogram of Values in the 2nd Column')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>noise_perturbation = final_data_tensor[:, :, 3].flatten()[labels == 0]\nsignal_perturbation = final_data_tensor[:, :, 3].flatten()[labels == 1]\n\nprint(f\"The noise binding max is {noise_perturbation.max()} and the min is {noise_perturbation.min()}\")\nprint(f\"the noise min is {noise_perturbation.min()}\")\nprint(f\"the noise mean is {noise_perturbation.mean()} and the std is {noise_perturbation.std()}\")\nprint(f\"The signal binding max is {signal_perturbation.max()} and the min is {signal_perturbation.min()}\")\nprint(f\"the signal min is {signal_perturbation.min()}\")\nprint(f\"the signal mean is {signal_perturbation.mean()} and the std is {signal_perturbation.std()}\")\n</code></pre> <pre>\n<code>The noise binding max is 3.423511505126953 and the min is -3.506139039993286\nthe noise min is -3.506139039993286\nthe noise mean is 0.010617653839290142 and the std is 0.988001823425293\nThe signal binding max is 6.107701301574707 and the min is -6.406703948974609\nthe signal min is -6.406703948974609\nthe signal mean is -0.011303802020847797 and the std is 3.136451482772827\n</code>\n</pre> <pre><code># Plotting\nplt.figure(figsize=(10, 6))\nplt.hist(noise_perturbation, bins=30, alpha=0.5, label='Label 0', color='orange')\nplt.hist(signal_perturbation, bins=30, alpha=0.5, label='Label 1', color='blue')\nplt.title('Histogram of Values in the 2nd Column')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n</code></pre> <pre><code># plot the binding effects vs the perturbation effects\n# color the points by the label\n# make sure the labels are categorical\n# label 0 should be blue while label 1 should be orange\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.scatter(final_data_tensor[:, :, 1].flatten(), final_data_tensor[:, :, 3].flatten().abs(), c=['orange' if x == 0 else 'blue' for x in labels])\nplt.title('Binding Effects vs Perturbation Effects')\nplt.xlabel('Binding Effects')\nplt.ylabel('Perturbation Effects')\nplt.xlim(0,100)\nplt.show()\n</code></pre> <pre><code># in this case, select the TF binding data that corresponds with the effect data\n# which we wish to produce. use the .unsqueeze(1) method to add the TF dimension\n# after selecting the TF\nperturbation_effects_tf_influenced = generate_perturbation_effects(\n    binding_data_tensor, \n    max_mean_adjustment=3.0, # try 0.1, 3.0, and 10.0\n    signal_mean=5.0, # try 3.0, 5.0, or 10.0\n    noise_mean=0.0, # try adjusting this\n)\nperturbation_pvalue_tf_influenced = torch.zeros_like(perturbation_effects_tf_influenced)\nfor col_idx in range(perturbation_effects_tf_influenced.shape[1]):\n    col = perturbation_effects_tf_influenced[:, col_idx]\n    col_pvals = generate_pvalues(col)\n    perturbation_pvalue_tf_influenced[:, col_idx] = col_pvals\n\nperturbation_effects_tensor_tf_influened = perturbation_effects_tf_influenced.unsqueeze(-1)\nperturbation_pvalues_tensor_tf_influenced = perturbation_pvalue_tf_influenced.unsqueeze(-1)\n\nfinal_data_tensor_tf_influenced = torch.cat(\n    (binding_data_tensor,\n     perturbation_effects_tensor_tf_influened,\n     perturbation_pvalues_tensor_tf_influenced), \n    dim=2)\n\n# Verify the shape\nprint(\"Shape of the final data tensor:\", final_data_tensor.shape)\n</code></pre> <pre>\n<code>Shape of the final data tensor: torch.Size([1000, 10, 5])\n</code>\n</pre> <pre><code># Plotting. Note that the 'noise' group effects are still range from 0 to 3\n\nplt.figure(figsize=(10, 6))\nplt.scatter(final_data_tensor_tf_influenced[:, :, 1].flatten(), final_data_tensor_tf_influenced[:, :, 3].flatten().abs(), c=['orange' if x == 0 else 'blue' for x in labels])\nplt.title('Binding Effects vs Perturbation Effects')\nplt.xlabel('Binding Effects')\nplt.ylabel('Perturbation Effects')\n\nlegend_labels = ['Bound', 'Unbound']\ncolors = ['blue', 'orange']\nlegend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in colors]\nplt.legend(legend_handles, legend_labels)\n\nplt.show()\n</code></pre>"},{"location":"tutorials/generate_in_silico_data/#generating-in-silico-data","title":"Generating in silico data","text":""},{"location":"tutorials/generate_in_silico_data/#step-1","title":"Step 1:","text":"<p>The first step is to generate a gene population, or set of gene populations. A gene population is simply a class that stores a 1D tensor called <code>labels</code>. <code>labels</code> is a boolean vector where 1 means the gene is part of the signal group (a gene which is both bound and responsive to the TF) while 0 means the gene is part of the background or noise group. The length of <code>labels</code> is the number of genes in the population, and the index should be considered the unique gene identifier. In other words, the indicies should never change.</p>"},{"location":"tutorials/generate_in_silico_data/#step-2","title":"Step 2:","text":"<p>The second step is to generate binding data from the gene population(s).</p>"},{"location":"tutorials/generate_in_silico_data/#step-3-generate-perturbation-data","title":"Step 3: Generate perturbation data.","text":"<p>It is important to understand that there are four possible ways we provide for you to generate perturbation data. 1. No Mean Adjustment 2. Standard Mean Adjustment 3. Mean adjustment dependent on all TFs bound to gene in question 4. Mean adjustment dependent on binary relationships between bound and unbound TFs to gene in question.</p>"},{"location":"tutorials/generate_in_silico_data/#method-1-generating-perturbation-data-with-no-mean-adjustment","title":"Method 1: Generating perturbation data with no mean adjustment","text":"<p>If you don\u2019t pass in a value for <code>max_mean_adjustment</code> to <code>generate_perturbation_effects</code> it will default to zero, meaning the means of the perturbation effects will not be adjusted in any way and will all be equal to <code>signal_mean</code> (deault is 3.0) for bound TF-gene pairs and <code>noise_mean</code> (default is 0.0) for unbound TF-gene pairs.</p>"},{"location":"tutorials/generate_in_silico_data/#method-2-generating-perturbation-data-with-a-simple-mean-adjustment","title":"Method 2: Generating perturbation data with a simple mean adjustment","text":"<p>If you do pass in a nonzero value for <code>max_mean_adjustment</code>, the means of bound gene-TF pairs will be adjusted by up to a maximum of <code>max_mean_adjustment</code>. Note that instead of passing in one column (corresponding to one TF) of the binding data tensor at a time, we instead pass in the entire binding data tensor at once. This syntactic difference is just a result of how our mean adjustment functions requires the entire matrix of all genes and TFs as opposed to being able to operate on one column at once. Using this data generation method, we adust the mean of any TF that is bound to a gene.</p>"},{"location":"tutorials/generate_in_silico_data/#method-3-generating-perturbation-data-with-a-mean-adjustment-dependent-on-which-tfs-are-bound-to-gene","title":"Method 3: Generating Perturbation Data with a mean adjustment dependent on which TFs are bound to gene","text":"<p>You are also able to specify a dictionary of TF relationships. Passing in this dictionary in combination with using our <code>perturbation_effect_adjustment_function_with_tf_relationships</code> mean adjustment function alows for you to only adjust the means of perturbation effects if the TF in the TF-gene pair in question is bound AND all other TFs associated with that TF are bound to the same gene. To associate a TF with another TF, put its index in the list of TFs corresponding to the other TF\u2019s index in the tf_relationships dictionary.</p>"},{"location":"tutorials/generate_in_silico_data/#method-4-generating-perturbation-data-with-a-mean-adjustment-dependent-on-boolean-relationships-between-tfs","title":"Method 4: Generating Perturbation Data with a mean adjustment dependent on boolean relationships between TFs","text":"<p>(see the documentation in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> for more information on <code>And()</code> and <code>Or()</code>) </p> <p>This is a more advanced version of method 3 where instead of only specifying direct dependencies you can specify logical relations that must be satisfied for a gene-TF pair\u2019s perturbation effect value to be adjusted. For example, in the below example we only adjust the mean of TF 3 for each gene if TF 3 is bound and (7 || 9) &amp;&amp; (6 &amp;&amp; 7) are bound. </p>"},{"location":"tutorials/generate_in_silico_data/#step-4-assemble","title":"Step 4: Assemble","text":"<p>The final step is to assemble the data into a single tensor. Here is one way. The order of the matrix in the last dimension is:</p> <ol> <li>signal/noise label</li> <li>binding effect</li> <li>binding pvalue</li> <li>perturbation effect</li> <li>perturbation pvalue</li> </ol> <p>For simplicity\u2019s sake, we will use the perturbation effect data we generated with no mean adjustment. However you can assemble the data using perturbation effect data generated from any of the 4 methods we covered above.</p>"},{"location":"tutorials/generate_in_silico_data/#sanity-checks","title":"Sanity checks","text":"<p>Ensure that the generated data matches expectations.</p>"},{"location":"tutorials/generate_in_silico_data/#the-signalnoise-ratios-should-match-exactly-the-initial-signal-ratio","title":"The signal/noise ratios should match exactly the initial signal ratio","text":""},{"location":"tutorials/generate_in_silico_data/#binding-effect-distributions-should-match-expectations","title":"Binding effect distributions should match expectations","text":""},{"location":"tutorials/generate_in_silico_data/#perturbation-effect-distribtuions-should-match-expectations","title":"Perturbation effect distribtuions should match expectations","text":""},{"location":"tutorials/generate_in_silico_data/#the-binding-effects-should-be-positively-correlated-with-the-perturbaiton-effects","title":"The binding effects should be positively correlated with the perturbaiton effects","text":""},{"location":"tutorials/generate_in_silico_data/#re-generate-data-with-an-explicit-relationship-between-a-give-tfs-binding-and-perturbation-effects","title":"Re-generate data with an explicit relationship between a give TF\u2019s binding and perturbation effects","text":""},{"location":"tutorials/hyperparameter_sweep/","title":"Hyperparameter Sweep","text":"<p>This notebook introduces how to perform a hyperparameter sweep to find the best hyperparameters for our model using the Optuna library. Feel free to modify the objective function if you would like to test other hyperparameters or values.</p> <pre><code># imports \nimport argparse\nfrom argparse import Namespace\n\nfrom pytorch_lightning import Trainer, LightningModule, seed_everything\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\nfrom torchsummary import summary\n\nfrom yeastdnnexplorer.data_loaders.synthetic_data_loader import SyntheticDataLoader\nfrom yeastdnnexplorer.ml_models.simple_model import SimpleModel\nfrom yeastdnnexplorer.ml_models.customizable_model import CustomizableModel\n\nimport optuna\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# set random seed for reproducability\nseed_everything(42)\n</code></pre> <pre>\n<code>Seed set to 42\n</code>\n</pre> <pre>\n<code>42</code>\n</pre> <p>Here we define loggers and checkpoints for our model. Checkpoints tell pytorch when to save instances of the model (that can be loaded and inspected later) and loggers tell pytorch how to format the metrics that the model logs during its training. </p> <pre><code># Checkpoint to save the best version of model (during the entire training process) based on the metric passed into \"monitor\"\nbest_model_checkpoint = ModelCheckpoint(\n    monitor=\"val_mse\",  # You can modify this to save the best model based on any other metric that the model you're testing tracks and reports\n    mode=\"min\",\n    filename=\"best-model-{epoch:02d}-{val_loss:.2f}.ckpt\",\n    save_top_k=1,  # Can modify this to save the top k models\n)\n\n# Callback to save checkpoints every 2 epochs, regardless of performance\nperiodic_checkpoint = ModelCheckpoint(\n    filename=\"periodic-{epoch:02d}.ckpt\",\n    every_n_epochs=2,\n    save_top_k=-1,  # Setting -1 saves all checkpoints\n)\n\n# define loggers for the model\ntb_logger = TensorBoardLogger(\"logs/tensorboard_logs\")\ncsv_logger = CSVLogger(\"logs/csv_logs\")\n</code></pre> <p>Now we perform our hyperparameter sweep using the Optuna library. To do this, we need to define an objective function that returns a scalar value. This scalar value will be the value that our sweep is attempting to minimize. We train one instance of our model inside each call to the objective function (each model on each iteration will use a different selection of hyperparameters). In our objective function, we return the validation mse associated with the instance of the model. This is because we would like to find the combination of hyperparameters that leads to the lowest validation mse. We use validation mse instead of test mse since we do not want to risk fitting to the test data at all while tuning hyperparameters.</p> <p>If you\u2019d like to try different hyperparameters, you just need to modify the list of possible values corresponding to the hyperparameter in question.</p> <p>If you\u2019d like to run the hyperparamter sweep on real data instead of synthetic data, simply swap out the synthetic data loader for the real data loader.</p> <pre><code># on each call to the objective function, it will choose a hyperparameter value from each of the suggest_categorical arrays and pass them into the model\n    # this allows us to test many different hyperparameter configurations during our sweep\n\ndef objective(trial):\n    # model hyperparameters\n    lr = trial.suggest_categorical(\"lr\", [0.01])\n    hidden_layer_num = trial.suggest_categorical(\"hidden_layer_num\", [1, 2, 3, 5])\n    activation = trial.suggest_categorical(\n        \"activation\", [\"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\"]\n    )\n    optimizer = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\"])\n    L2_regularization_term = trial.suggest_categorical(\n        \"L2_regularization_term\", [0.0, 0.1]\n    )\n    dropout_rate = trial.suggest_categorical(\n        \"dropout_rate\", [0.0, 0.5]\n    )\n\n    # data module hyperparameters\n    batch_size = trial.suggest_categorical(\"batch_size\", [32])\n\n    # training hyperparameters\n    max_epochs = trial.suggest_categorical(\n        \"max_epochs\", [1]\n    ) # default is 10\n\n    # defining what to pass in for the hidden layer sizes list based on the number of hidden layers\n    hidden_layer_sizes_configurations = {\n        1: [[64], [256]],\n        2: [[64, 32], [256, 64]],\n        3: [[256, 128, 32], [512, 256, 64]],\n        5: [[512, 256, 128, 64, 32]],\n    }\n    hidden_layer_sizes = trial.suggest_categorical(\n        f\"hidden_layer_sizes_{hidden_layer_num}_layers\",\n        hidden_layer_sizes_configurations[hidden_layer_num],\n    )\n\n    print(\"=\" * 70)\n    print(\"About to create model with the following hyperparameters:\")\n    print(f\"lr: {lr}\")\n    print(f\"hidden_layer_num: {hidden_layer_num}\")\n    print(f\"hidden_layer_sizes: {hidden_layer_sizes}\")\n    print(f\"activation: {activation}\")\n    print(f\"optimizer: {optimizer}\")\n    print(f\"L2_regularization_term: {L2_regularization_term}\")\n    print(f\"dropout_rate: {dropout_rate}\")\n    print(f\"batch_size: {batch_size}\")\n    print(f\"max_epochs: {max_epochs}\")\n    print(\"\")\n\n    # create data module\n    data_module = SyntheticDataLoader(\n        batch_size=batch_size,\n        num_genes=4000,\n        signal_mean=3.0,\n        signal=[0.5] * 10,\n        n_sample=[1, 2, 2, 4, 4],\n        val_size=0.1,\n        test_size=0.1,\n        random_state=42,\n        max_mean_adjustment=3.0,\n    )\n\n    num_tfs = sum(data_module.n_sample)  # sum of all n_sample is the number of TFs\n\n    # create model\n    model = CustomizableModel(\n        input_dim=num_tfs,\n        output_dim=num_tfs,\n        lr=lr,\n        hidden_layer_num=hidden_layer_num,\n        hidden_layer_sizes=hidden_layer_sizes,\n        activation=activation,\n        optimizer=optimizer,\n        L2_regularization_term=L2_regularization_term,\n        dropout_rate=dropout_rate,\n    )\n\n    # create trainer\n    trainer = Trainer(\n        max_epochs=max_epochs,\n        deterministic=True,\n        accelerator=\"cpu\",\n        # callbacks and loggers are commented out for now since running a large sweep would generate an unnecessarily huge amount of checkpoints and logs\n        # callbacks=[best_model_checkpoint, periodic_checkpoint],\n        # logger=[tb_logger, csv_logger],\n    )\n\n    # train model\n    trainer.fit(model, data_module)\n\n    # get best validation loss from the model\n    return trainer.callback_metrics[\"val_mse\"]\n</code></pre> <p>Now we define an optuna study, which represents our hyperparameter sweep. It will run the objective function n_trials times and choose the model that gave the best val_mse across all of those trials with different hyperparameters. Note that this will create a very large amount of output as it will show training stats for every model. This is why we print out the best params and loss in a separate cell.</p> <pre><code>STUDY_NAME = \"CustomizableModelHyperparameterSweep3\"\nNUM_TRIALS = 5 # you will need a lot more than 5 trials if you have many possible combinations of hyperparams\n\n# Perform hyperparameter optimization using Optuna\nstudy = optuna.create_study(\n    direction=\"minimize\", # we want to minimize the val_mse\n    study_name=STUDY_NAME,\n    # storage=\"sqlite:///db.sqlite3\", # you can save the study results in a database if you'd like, this is needed if you want to try and use the optuna dashboard library to dispaly results\n)\nstudy.optimize(objective, n_trials=NUM_TRIALS)\n\n# Get the best hyperparameters and their corresponding values\nbest_params = study.best_params\nbest_loss = study.best_value\n</code></pre> <p>Print out the best hyperparameters and the val_mse assocaited with the model with the best hyperparameters.</p> <pre><code>print(\"RESULTS\" + (\"=\" * 70))\nprint(f\"Best hyperparameters: {best_params}\")\nprint(f\"Best loss: {best_loss}\")\n</code></pre> <p>And that\u2019s it! Now you could take what you found to be the best hyperparameters and train a model with them for many more epochs. The Optuna Documentation will be a helpful resource if you\u2019d like to add more to this notebook or the hyperparam sweep functions</p>"},{"location":"tutorials/lightning_crash_course/","title":"Lightning Crash Course","text":"<pre><code># imports\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n\nfrom yeastdnnexplorer.data_loaders.synthetic_data_loader import SyntheticDataLoader\nfrom yeastdnnexplorer.data_loaders.real_data_loader import RealDataLoader\nfrom yeastdnnexplorer.ml_models.simple_model import SimpleModel\nfrom yeastdnnexplorer.ml_models.customizable_model import CustomizableModel\n</code></pre> <p>In Pytorch Lightning, the data is kept completely separate from the models. This allows for you to easy train a model using different datasets or train different models on the same dataset. <code>DataModules</code> encapsulate all the logic of loading in a specific dataset and splitting into training, testing, and validation sets. In this project, we have two data loaders defined: <code>SyntheticDataLoader</code> for the in silico data (which takes in many parameters that allow you to specify how the data is generated) and <code>RealDataLoader</code> which contains all of the logic for loading in the real experiment data and putting it into a form that the models expect.</p> <p>Once you decide what model you want to train and what dataModule you want to use, you can bundle these with a <code>Trainer</code> object to train the model on the dataset.</p> <p>If you\u2019d like to learn more about the models and dataModules we\u2019ve defined, there is extensive documentation in each of the files that explains each method\u2019s purpose.</p> <pre><code># define an instance of our simple linear baseline model\nmodel = SimpleModel(\n    input_dim=10,\n    output_dim=10,\n    lr=1e-2,\n)\n\n# define an instance of the synthetic data loader\n# see the constructor for the full list of params and their explanations\ndata_module = SyntheticDataLoader(\n    batch_size=32,\n    num_genes=3000,\n    signal=[0.5] * 5,\n    n_sample=[1, 1, 2, 2, 4],\n    val_size=0.1,\n    test_size=0.1,\n    signal_mean=3.0,\n)\n\n# define a trainer instance\ntrainer = Trainer(\n    max_epochs=10,\n    deterministic=True,\n    accelerator=\"cpu\", # change to \"gpu\" if you have access to one\n)\n\n# train the model\ntrainer.fit(model, data_module)\n\n# test the model (recall that data_module specifies the train / test split, we don't need to do it explicitly here)\ntest_results = trainer.test(model, data_module)\nprint(test_results)\n\n</code></pre> <p>It\u2019s very easy to train the same model on a different dataset, for example if we want to use real world data we can just swap to the data module that we\u2019ve defined for the real world data.</p> <pre><code># we need to redefine a new instance with the same params unless we want it to pick up where it left off\nnew_model = SimpleModel(\n    input_dim=30,  # note that the input and output dims are equal to the num TFs in the dataset\n    output_dim=30,\n    lr=1e-2,\n)\n\nreal_data_module = RealDataLoader(\n    batch_size=32,\n    val_size=0.1,\n    test_size=0.1,\n    data_dir_path=\"../../data/init_analysis_data_20240409/\", # note that this is relative to where real_data_loader.py is\n    perturbation_dataset_title=\"hu_reimann_tfko\",\n)\n\n# we also have to define a new trainer instance, not really sure why but it seems to be necessary\ntrainer = Trainer(\n    max_epochs=10,\n    deterministic=True,\n    accelerator=\"cpu\", # change to \"gpu\" if you have access to one\n)\n\ntrainer.fit(new_model, real_data_module)\ntest_results = trainer.test(new_model, real_data_module)\nprint(test_results)\n</code></pre> <p>If we wanted to do the same thing with our more complex and customizable <code>CustomizableModel</code> (which allows you to pass in many params like the number of hidden layers, dropout rate, choice of optimizer, etc) the code would look identical to above except that we would be initializing a <code>CustomizableModel</code> instead of a <code>SimpleModel</code>. See the documentation in <code>customizable_model.py</code> for more</p> <pre><code># this will be used to save the model checkpoint that performs the best on the validation set\nbest_model_checkpoint = ModelCheckpoint(\n    monitor=\"val_mse\", # we can depend on any metric we want\n    mode=\"min\",\n    filename=\"best-model-{epoch:02d}-{val_loss:.2f}\",\n    save_top_k=1, # we can save more than just the top model if we want\n)\n\n# Callback to save checkpoints every 2 epochs, regardless of model performance\nperiodic_checkpoint = ModelCheckpoint(\n    filename=\"periodic-{epoch:02d}\",\n    every_n_epochs=2,\n    save_top_k=-1,  # Setting -1 saves all checkpoints  \n)\n\n# csv logger is a very basic logger that will create a csv file with our metrics as we train\ncsv_logger = CSVLogger(\"logs/csv_logs\")  # we define the directory we want the logs to be saved in\n\n# tensorboard logger is a more advanced logger that will create a directory with a bunch of files that can be visualized with tensorboard\n# tensorboard is a library that can be ran via the command line, and will create a local server that can be accessed via a web browser\n# that displays the training metrics in a more interactive way (on a dashboard)\n# you can run tensorboard by running the command `tensorboard --logdir=path/to/log/dir` in the terminal\ntb_logger = TensorBoardLogger(\"logs/tensorboard_logs\", name=\"test-run-2\")\n\n# If we wanted to use these checkpoints and loggers, we would pass them to the trainer like so:\ntrainer_with_checkpoints_and_loggers = Trainer(\n    max_epochs=10,\n    deterministic=True,\n    accelerator=\"cpu\",\n    callbacks=[best_model_checkpoint, periodic_checkpoint],\n    logger=[csv_logger, tb_logger],\n)\n</code></pre> <pre><code># Load a model from a checkpoint\n# We can load a model from a checkpoint like so:\npath_to_checkpoint = \"example/path/not/real.ckpt\"\n\n# note that we need to use the same model class that was used to save the checkpoint\nmodel = SimpleModel.load_from_checkpoint(path_to_checkpoint)\n\n# we can load the model and continue training from where it left off\ntrainer.fit(model, data_module)\n\n# we could also load the model and test it\ntest_results = trainer.test(model, data_module)\n\n# we could also load the model and make predictions\npredictions = model(data_module.test_dataloader())\n</code></pre>"},{"location":"tutorials/lightning_crash_course/#lightning-crash-course","title":"Lightning Crash Course","text":"<p>This project uses the PyTorch Lightning Library to define and train the machine learning models. PyTorch Lightning is built on top of pytorch, and it abstracts away some of the setup and biolerplate for models (such as writing out training loops). In this notebook, we provide a brief introduction to how to use the models and dataModules we\u2019ve defined to train models.</p>"},{"location":"tutorials/lightning_crash_course/#checkpointing-logging","title":"Checkpointing &amp; Logging","text":"<p>PyTorch lightning gives us the power to define checkpoints and loggers that will be used during training. Checkpoints will save checkpoints of your model during training. In the following code, we define a checkpoint that saves the model\u2019s state when it produced the lowest validation mean squared error on the validation set during training. We also define another checkpoint to periodically save a checkpoint of the model after every 2 training epochs. These checkpoints are powerful because they can be reloaded later. You can continue training a model after loading its checkpoint or you can test the model checkpoint on new data.</p> <p>Loggers are responsible for saving metrics about the model as it is training for us to look at later. We define several loggers to track this data. See the comments above the Tensorboard logger to see how to use Tensorboard to visualize the metrics as the model trains</p> <p>To use checkpoints and loggers, we have to pass them into the Trainer object that we use to train the model with a dataModule. </p> <p>There are many more types of checkpoints and loggers you can create and use, PyTorch Lightning\u2019s documentation is very helpful here</p>"},{"location":"tutorials/lightning_crash_course/#loading-in-and-using-a-checkpoint","title":"Loading in and using a Checkpoint","text":""},{"location":"tutorials/testing_model_metrics/","title":"Testing Model Metrics","text":"<p>In this notebook, we run several simple experiments to gain a deeper understanding of the metrics that we use to evaluate our models and how they respond to changes in the parameters we use for generating our in silico data.</p> <pre><code># imports\nimport torch\n\nfrom pytorch_lightning import Trainer, LightningModule\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n\nfrom yeastdnnexplorer.data_loaders.synthetic_data_loader import SyntheticDataLoader\nfrom yeastdnnexplorer.ml_models.simple_model import SimpleModel\n\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(42)  # For CPU\ntorch.cuda.manual_seed_all(42)  # For all CUDA devices\n</code></pre> <p>Define checkpoints and loggers for the models</p> <pre><code># define checkpoints for the model\n# tells it when to save snapshots of the model during training\n# Callback to save the best model based on validation loss\nbest_model_checkpoint = ModelCheckpoint(\n    monitor=\"val_mse\",\n    mode=\"min\",\n    filename=\"best-model-{epoch:02d}-{val_loss:.2f}\",\n    save_top_k=1,\n)\n\n# Callback to save checkpoints every 5 epochs, regardless of performance\nperiodic_checkpoint = ModelCheckpoint(\n    filename=\"periodic-{epoch:02d}\",\n    every_n_epochs=2,\n    save_top_k=-1,  # Setting -1 saves all checkpoints\n)\n\n# configure loggers\ntb_logger = TensorBoardLogger(\"logs/tensorboard_logs\")\ncsv_logger = CSVLogger(\"logs/csv_logs\")\n</code></pre> <p>Here we define a helper function that will generate data and train a simple linear model with all of the given parameters, print the test results of the model, and return the trained model and its test results. This will allow us to easily run experiments where we compare model performance while tweaking data or model parameters.</p> <pre><code>def train_simple_model_with_params(\n    batch_size: int,\n    lr: float,\n    max_epochs: int,\n    using_random_seed: bool,\n    accelerator: str,\n    num_genes: int,\n    signal_mean: float,\n    val_size: float,\n    test_size: float,\n    signal: list[float],\n    n_sample: list[int],\n    max_mean_adjustment: float,\n) -&amp;gt; LightningModule:\n    data_module = SyntheticDataLoader(\n        batch_size=batch_size,\n        num_genes=num_genes,\n        signal_mean=signal_mean,\n        signal=signal,  # old: [0.1, 0.15, 0.2, 0.25, 0.3],\n        n_sample=n_sample,  # sum of this is num of tfs\n        val_size=val_size,\n        test_size=test_size,\n        random_state=42,\n        max_mean_adjustment=max_mean_adjustment,\n    )\n\n    num_tfs = sum(data_module.n_sample)  # sum of all n_sample is the number of TFs\n\n    model = SimpleModel(input_dim=num_tfs, output_dim=num_tfs, lr=lr)\n    trainer = Trainer(\n        max_epochs=max_epochs,\n        deterministic=using_random_seed,\n        accelerator=accelerator,\n        # callbacks=[best_model_checkpoint, periodic_checkpoint],\n        # logger=[tb_logger, csv_logger],\n    )\n    trainer.fit(model, data_module)\n    test_results = trainer.test(model, datamodule=data_module)\n    print(\"Printing test results...\")\n    print(\n        test_results\n    )  # this prints all metrics that were logged during the test phase\n\n    return model, test_results\n</code></pre> <pre><code>signal_means = [0.5, 1.0, 2.0, 3.0, 5.0]\ntest_mses = []\nfor signal_mean in signal_means:\n    model, test_results = train_simple_model_with_params(\n        batch_size=32,\n        lr=0.01,\n        max_epochs=10,\n        using_random_seed=True,\n        accelerator=\"cpu\",\n        num_genes=1000,\n        val_size=0.1,\n        test_size=0.1,\n        signal=[0.5] * 5,\n        n_sample=[1, 1, 2, 2, 4],  # sum of this is num of tfs\n        signal_mean=signal_mean,\n        max_mean_adjustment=0.0\n    )\n    test_mses.append(test_results[0][\"test_mse\"])\n</code></pre> <p>Plot Results</p> <pre><code>plt.plot(signal_means, test_mses, marker=\"o\")\nplt.xlabel(\"Signal Mean\")\nplt.xticks(signal_means, rotation=45)\nplt.yticks(test_mses)\nplt.ylabel(\"Test MSE\")\nplt.title(\"Test MSE as a function of Signal Mean\")\nplt.show()\n</code></pre> <pre><code>signal_noise_ratios = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9]\ntest_mses = []\n\nfor signal_noise_ratio in signal_noise_ratios:\n    model, test_results = train_simple_model_with_params(\n        batch_size=32,\n        lr=0.01,\n        max_epochs=10,\n        using_random_seed=True,\n        accelerator=\"cpu\",\n        num_genes=1000,\n        val_size=0.1,\n        test_size=0.1,\n        signal=[signal_noise_ratio] * 5,\n        n_sample=[1, 1, 2, 2, 4],\n        signal_mean=3.0,\n        max_mean_adjustment=0.0\n    )\n    print(test_results)\n    test_mses.append(test_results[0][\"test_mse\"])\n</code></pre> <pre><code>plt.plot(signal_noise_ratios, test_mses, marker=\"o\")\nplt.xlabel(\"Percentage of Data in Signal Group\")\nplt.ylabel(\"Test MSE\")\nplt.xticks(signal_noise_ratios, rotation=45)\nplt.yticks(test_mses)\nplt.title(\"Test MSE as a function of signal/noise ratio (signal mean = 3.0)\")\nplt.show()\n</code></pre> <pre><code># these params will be consistent for both datasets\nnum_genes = 3000\nval_size = 0.1\ntest_size = 0.1\nsignal = [0.5] * 5\nn_sample = [1, 1, 2, 2, 4]\nrandom_state = 42\n\n# the first data loader will load a dataset with a small scale and a small bound mean\nsmall_scale_and_mean_dataloader = SyntheticDataLoader(\n    num_genes=num_genes,\n    signal=signal, \n    n_sample=n_sample,\n    val_size=val_size,\n    test_size=test_size,\n    random_state=random_state,\n    signal_mean=1.0,\n    max_mean_adjustment=1.0\n)\n\n# the second data loader will generate a dataset with a large scale and a large bound mean\nlarge_scale_and_mean_dataloader = SyntheticDataLoader(\n    num_genes=num_genes,\n    signal=signal, \n    n_sample=n_sample,\n    val_size=val_size,\n    test_size=test_size,\n    random_state=random_state,\n    signal_mean=10.0,\n    max_mean_adjustment=10.0\n)\n\nnum_tfs = sum(n_sample)  # sum of all n_sample is the number of TFs\n\nmodel = SimpleModel(input_dim=num_tfs, output_dim=num_tfs, lr=0.01)\ntrainer = Trainer(\n    max_epochs=10,\n    deterministic=True,\n    accelerator='cpu',\n    # callbacks=[best_model_checkpoint, periodic_checkpoint],\n    # logger=[tb_logger, csv_logger],\n)\n\ntrainer.fit(model, small_scale_and_mean_dataloader)\nsmall_test_results = trainer.test(model, datamodule=small_scale_and_mean_dataloader)\nprint(\"Printing small test results...\")\nprint(small_test_results)\n\n\ntrainer.fit(model, large_scale_and_mean_dataloader)\nlarge_test_results = trainer.test(model, datamodule=large_scale_and_mean_dataloader)\nprint(\"Printing large test results...\")\nprint(large_test_results)\n</code></pre>"},{"location":"tutorials/testing_model_metrics/#experiment-1","title":"Experiment 1","text":"<p>Now we can use this function to run simple experiments, like testing how the model\u2019s test mse changes when we tweak the mean of the bound genes while holding all other parameters the same. For simplicity, we will not be performing any mean adjustments while generating the data, but we could modify this in the future by incresing the max_mean_adjustment (to use a normal mean adjustment) or adding onto our experiment function to take in our special mean adjustment functions (to use either of the special dependent mean adjustment logic that we\u2019ve defined, see <code>generate_in_silico_data.ipynb</code> for more about this).</p> <p>Note that this will create a lot of output since we are training several models, so we create the plot in a separate cell.</p>"},{"location":"tutorials/testing_model_metrics/#experiment-2","title":"Experiment 2","text":"<p>We can run a similar experiment where we test the effect of the bound / unbound ratio (aka signal / noise ratio) on the model\u2019s MSE</p>"},{"location":"tutorials/testing_model_metrics/#experiment-3","title":"Experiment 3","text":"<p>Here we run a little experiment to verify that our smse (standardized mean squared error) metric is actually scale and mean invariant (ie doesn\u2019t depend on the scale or mean of the data so long as the variance is roughly the same). Note that this isn\u2019t a perfect experiment, as increasing the max mean adjustment (and therefore the scale) will increase the variance by a factor as a result of how our in silico data generation functions work, so there will definitely be a little difference in smse values, but the difference in mse and mae should be a much larger percentage.</p> <p>We will train and test two models that are exactly the same except that one is trained on a dataset with a small bound mean and mean adjustment and one is trained on a dataset with a large bound mean adn mean adjustment. This will give the two datasets drastically different scales and means. Unfortunately, it will also give them slightly different variances which should cause a slight difference in smse. But again it should be a much smaller percentage difference than the difference between mses and maes</p>"},{"location":"tutorials/visualizing_and_testing_data_generation_methods/","title":"Visualizing and Testing Data Generation Methods","text":"<p>In this notebook, we will run an experiment to display the average perturbation effect values that we generate with the 4 different methods we have for perturbation effect generation (other than the method for generating the perturbation effect values, we will be holding everything else the same). </p> <p>Recall that we have 4 methods for generating perturbation effect data (see <code>generate_in_silico_data.ipynb</code> for more information on these): 1. No Mean Adjustment 2. Standard Mean Adjustment 3. Mean adjustment dependent on all TFs bound to gene in question 4. Mean adjustment dependent on binary relationships between bound and unbound TFs to gene in question.</p> <p>After understanding what the generated data looks like for each of these methods, we will perform another experiment where we train the same model on data generated with each of these methods and compare the model\u2019s performance to a simple linear model.</p> <pre><code># imports\nfrom yeastdnnexplorer.probability_models.generate_data import (generate_gene_population, \n                                                               generate_binding_effects,\n                                                               generate_pvalues,\n                                                               generate_perturbation_effects)\n\nimport torch\nimport matplotlib.pyplot as plt\n\nfrom yeastdnnexplorer.probability_models.relation_classes import Relation, And, Or\nfrom yeastdnnexplorer.probability_models.generate_data import (\n    default_perturbation_effect_adjustment_function,\n    perturbation_effect_adjustment_function_with_tf_relationships,\n    perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic\n)\n\nfrom pytorch_lightning import Trainer, LightningModule, seed_everything\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\nfrom torchsummary import summary\n\nfrom yeastdnnexplorer.data_loaders.synthetic_data_loader import SyntheticDataLoader\nfrom yeastdnnexplorer.ml_models.simple_model import SimpleModel\nfrom yeastdnnexplorer.ml_models.customizable_model import CustomizableModel\n\ntorch.manual_seed(42)  # For CPU\ntorch.cuda.manual_seed_all(42)  # For all CUDA devices\n</code></pre> <p>Generating the binding data will be the same as always, see <code>generate_in_silico_data.ipynb</code></p> <pre><code>n_genes = 3000\n\nsignal = [0.5, 0.5, 0.5, 0.5, 0.5]\nn_sample = [1, 1, 2, 2, 4]\n\n# this will be a list of length 10 with a GenePopulation object in each element\ngene_populations_list = []\nfor signal_proportion, n_draws in zip(signal, n_sample):\n    for _ in range(n_draws):\n        gene_populations_list.append(generate_gene_population(n_genes, signal_proportion))\n\n# Generate binding data for each gene population\nbinding_effect_list = [generate_binding_effects(gene_population)\n                     for gene_population in gene_populations_list]\n\n# Calculate p-values for binding data\nbinding_pvalue_list = [generate_pvalues(binding_data) for binding_data in binding_effect_list]\n\nbinding_data_combined = [torch.stack((gene_population.labels, binding_effect, binding_pval), dim=1)\n                         for gene_population, binding_effect, binding_pval\n                         in zip (gene_populations_list, binding_effect_list, binding_pvalue_list)]\n\n# Stack along a new dimension (dim=1) to create a tensor of shape [num_genes, num_TFs, 3]\nbinding_data_tensor = torch.stack(binding_data_combined, dim=1)\n</code></pre> <p>Now we define our experiment, this function will return the average perturbation effects (across n_iterations iterations) for each TF for a specific gene for each of the 4 data generation method we have at our disposal. Due to the randomness in the generated data, we need to find the averages over a number of iterations to get the true common values.</p> <p>We also need to define dictionaries of TF relationships for our third and fourth methods of generating perturbation data, see <code>generate_in_silico_data.ipynb</code> for an explanation of what these represent and how they are used / structured. The documentation in <code>generate_data.py</code> may be helpful as well.</p> <pre><code>tf_relationships = {\n    0: [1],\n    1: [8],\n    2: [5, 6],\n    3: [4],\n    4: [5],\n    5: [9],\n    6: [4],\n    7: [1, 4],\n    8: [6],\n    9: [4],\n}\n\ntf_relationships_dict_boolean_logic = {\n    0: [And(3, 4, 8), Or(3, 7), Or(1, 1)],\n    1: [And(5, Or(7, 8))],\n    2: [],\n    3: [Or(7, 9), And(6, 7)],\n    4: [And(1, 2)],\n    5: [Or(0, 1, 2, 8, 9)],\n    6: [And(0, Or(1, 2))],\n    7: [Or(2, And(5, 6, 9))],\n    8: [],\n    9: [And(6, And(3, Or(0, 9)))],\n}\n\ndef experiment(n_iterations = 10, GENE_IDX = 0):\n    print(\"Bound (1) and Unbound (0) Labels for gene \" + str(GENE_IDX) + \":\")\n    print(binding_data_tensor[GENE_IDX, :, 0])\n\n    num_tfs = sum(n_sample)\n\n    no_mean_adjustment_scores = torch.zeros(num_tfs)\n    normal_mean_adjustment_scores = torch.zeros(num_tfs)\n    dep_mean_adjustment_scores = torch.zeros(num_tfs)\n    boolean_logic_scores = torch.zeros(num_tfs)\n\n    # we generate perturbation effects for each TF on each iteration and then add them to the running totals\n    for i in range(n_iterations):\n        # Method 1: Generate perturbation effects without mean adjustment\n        perturbation_effects_list_no_mean_adjustment = [generate_perturbation_effects(binding_data_tensor[:, tf_index, :].unsqueeze(1), tf_index=0) \n                                                        for tf_index in range(sum(n_sample))]\n        perturbation_effects_list_no_mean_adjustment = torch.stack(perturbation_effects_list_no_mean_adjustment, dim=1)\n\n        # Method 2: Generate perturbation effects with normal mean adjustment\n        perturbation_effects_list_normal_mean_adjustment = generate_perturbation_effects(\n            binding_data_tensor, \n            max_mean_adjustment=10.0\n        )\n\n        # Method 3: Generate perturbation effects with dependent mean adjustment\n        perturbation_effects_list_dep_mean_adjustment = generate_perturbation_effects(\n            binding_data_tensor, \n            tf_relationships=tf_relationships,\n            adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships,\n            max_mean_adjustment=10.0,\n        )\n\n        # Method 4: Generate perturbation effects with binary relations between the TFs\n        perturbation_effects_list_boolean_logic = generate_perturbation_effects(\n            binding_data_tensor, \n            adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic,\n            tf_relationships=tf_relationships_dict_boolean_logic,\n            max_mean_adjustment=10.0,\n        )\n\n        # take absolute values since we only care about the magnitude of the effects\n        no_mean_adjustment_scores += abs(perturbation_effects_list_no_mean_adjustment[GENE_IDX, :])\n        normal_mean_adjustment_scores += abs(perturbation_effects_list_normal_mean_adjustment[GENE_IDX, :])\n        dep_mean_adjustment_scores += abs(perturbation_effects_list_dep_mean_adjustment[GENE_IDX, :])\n        boolean_logic_scores += abs(perturbation_effects_list_boolean_logic[GENE_IDX, :])\n\n        if (i + 1) % 5 == 0:\n            print(f\"iteration {i+1} completed\")\n\n    # divide by the number of iterations to get the averages\n    no_mean_adjustment_scores /= n_iterations\n    normal_mean_adjustment_scores /= n_iterations\n    dep_mean_adjustment_scores /= n_iterations\n    boolean_logic_scores /= n_iterations\n\n    return no_mean_adjustment_scores, normal_mean_adjustment_scores, dep_mean_adjustment_scores, boolean_logic_scores\n</code></pre> <p>Now we can run the experiment for n_iterations, I find that you should iterate at least 30 times, but closer to 100 is most ideal. This could take 1-5 minutes depending on your computer.</p> <pre><code>GENE_IDX = 0\nexperiment_results = experiment(n_iterations=50, GENE_IDX=GENE_IDX)\n</code></pre> <pre>\n<code>Bound (1) and Unbound (0) Labels for gene 0:\ntensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 1.])\niteration 5 completed\niteration 10 completed\niteration 15 completed\niteration 20 completed\niteration 25 completed\niteration 30 completed\niteration 35 completed\niteration 40 completed\niteration 45 completed\niteration 50 completed\n</code>\n</pre> <p>We now plot our results.</p> <pre><code>x_vals = list(range(sum(n_sample)))\nprint(\"Bound (signal) TFs for gene \" + str(GENE_IDX) + \" are: \" + str(binding_data_tensor[GENE_IDX, :, 0].nonzero().flatten().tolist()))\nprint(\"Unbound (noise) TFs for gene \" + str(GENE_IDX) + \" are: \" + str((1 - binding_data_tensor[GENE_IDX, :, 0]).nonzero().flatten().tolist()))\nprint(binding_data_tensor[GENE_IDX, :, 0])\nplt.figure(figsize=(10, 6))\n\n# Plot each set of experiment results with a different color\ncolors = ['red', 'green', 'blue', 'orange']\nfor index, results in enumerate(experiment_results):\n    plt.scatter(x_vals, results, color=colors[index])\n\nplt.title('Pertubation Effects for Gene ' + str(GENE_IDX) + ' with Different Adjustment Functions (averaged across 100 trials)')\nplt.xlabel('TF Index')\nplt.ylabel('Perturbation Effect Val')\nplt.xticks(x_vals)\nplt.grid(True)\nplt.legend(['No Mean Adjustment', 'Normal (non-dependent) Mean Adjust', 'Dependent Mean Adjustment', 'Boolean Logic Adjustment'])\nplt.show()\n</code></pre> <pre>\n<code>Bound (signal) TFs for gene 0 are: [3, 4, 5, 6, 7, 9]\nUnbound (noise) TFs for gene 0 are: [0, 1, 2, 8]\ntensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 1.])\n</code>\n</pre> <p>Recall that for the dependent mean adjustment, the TF in question must be bound and all of the TFs in its dependency array (in the tf_relationships dictionary) must be bound as well. This is why we do not adjust the mean for TF 7 despite it being bound, it depends on TF 1 and TF 4 both being bound, and TF1 is not bound.</p> <p>Similarly, for the boolean logic adjustment, we do not adjust the mean for 6 despite it being bound because it depends on (TF0 &amp;&amp; (TF1 || TF2)) being bound, and none of those 3 TFs are bound to the gene we are studying.</p> <p>Note that if you change GENE_IDX, the random seed, or any of the relationship dictionaris that this explanation will no longer apply to the data you are seeing in the plot.</p> <pre><code># define checkpoints and loggers\nbest_model_checkpoint = ModelCheckpoint(\n    monitor=\"val_mse\",\n    mode=\"min\",\n    filename=\"best-model-{epoch:02d}-{val_loss:.2f}\",\n    save_top_k=1,\n)\n\n# Callback to save checkpoints every 5 epochs, regardless of performance\nperiodic_checkpoint = ModelCheckpoint(\n    filename=\"periodic-{epoch:02d}\",\n    every_n_epochs=2,\n    save_top_k=-1,  # Setting -1 saves all checkpoints\n)\n\n# define loggers for the model\ntb_logger = TensorBoardLogger(\"logs/tensorboard_logs\")\ncsv_logger = CSVLogger(\"logs/csv_logs\")\n</code></pre> <p>We define a few helper functions to run our experiment. We make helper functions for things that will mostly be the same across each training loop so that we don\u2019t have to keep redefining them.</p> <pre><code>def get_data_module(max_mean_adjustment, adjustment_function = default_perturbation_effect_adjustment_function, tf_relationships_dict = {}):\n    return SyntheticDataLoader(\n        batch_size=32,\n        num_genes=4000,\n        signal_mean=3.0,\n        signal=[0.5] * 5,\n        n_sample=[1, 1, 2, 2, 4],  # sum of this is num of tfs\n        val_size=0.1,\n        test_size=0.1,\n        random_state=42,\n        max_mean_adjustment=max_mean_adjustment,\n        adjustment_function=adjustment_function,\n        tf_relationships=tf_relationships_dict,\n    )\n\ndef get_model(num_tfs):\n    return CustomizableModel(\n        input_dim=num_tfs,\n        output_dim=num_tfs,\n        lr=0.01,\n        hidden_layer_num=2,\n        hidden_layer_sizes=[64, 32],\n        activation=\"LeakyReLU\",\n        optimizer=\"RMSprop\",\n        L2_regularization_term=0.0,\n        dropout_rate=0.0,\n    )\n\ndef get_linear_model(num_tfs):\n    return SimpleModel(\n        input_dim=num_tfs,\n        output_dim=num_tfs,\n        lr=0.01\n    )\n\ndef get_trainer():\n    # uncomment callbacks or logggers if you would like checkpoints / logs\n    return Trainer(\n        max_epochs=10,\n        deterministic=True,\n        accelerator=\"cpu\",\n        # callbacks=[best_model_checkpoint, periodic_checkpoint],\n        # logger=[tb_logger, csv_logger],\n    )\n</code></pre> <pre><code># These lists will store the test results for different models and data generation methods\nmodel_mses = []\nlinear_model_test_mses = []\n</code></pre> <p>Train models on data generated with no mean adjustment</p> <pre><code>data_module = get_data_module(0.0)\nnum_tfs = sum(data_module.n_sample)\n\n# nonlinear model\nmodel = get_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(model, data_module)\ntest_results = trainer.test(model, datamodule=data_module)\nprint(\"Printing test results...\")\nprint(test_results)\nmodel_mses.append(test_results[0][\"test_mse\"])\n\n# linear model\nlinear_model = get_linear_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(linear_model, data_module)\ntest_results = trainer.test(linear_model, datamodule=data_module)\nprint(\"Printing linear model test results\")\nprint(test_results)\nlinear_model_test_mses.append(test_results[0][\"test_mse\"])\n</code></pre> <p>Train models on data generated with normal mean adjustments</p> <pre><code>data_module = get_data_module(3.0)\nnum_tfs = sum(data_module.n_sample)\n\n# nonlinear model\nmodel = get_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(model, data_module)\ntest_results = trainer.test(model, datamodule=data_module)\nprint(\"Printing test results...\")\nprint(test_results)\nmodel_mses.append(test_results[0][\"test_mse\"])\n\n# linear model\nlinear_model = get_linear_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(linear_model, data_module)\ntest_results = trainer.test(linear_model, datamodule=data_module)\nprint(\"Printing linear model test results\")\nprint(test_results)\nlinear_model_test_mses.append(test_results[0][\"test_mse\"])\n</code></pre> <p>Train model on data generated with dependent mean adjustments (method 3)</p> <pre><code># define dictionary of relations between TFs (see generate_in_silico_data.ipynb for an explanation of how this dict is defined / used)\ntf_relationships_dict = {\n    0: [1],\n    1: [8],\n    2: [5, 6],\n    3: [4],\n    4: [5],\n    5: [9],\n    6: [4],\n    7: [1, 4],\n    8: [6],\n    9: [4],\n}\n\ndata_module = get_data_module(\n    3.0, \n    adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships, \n    tf_relationships_dict=tf_relationships_dict\n)\nnum_tfs = sum(data_module.n_sample)\n\nprint(\"Number of TFs: \", num_tfs)\n\n# nonlinear model\nmodel = get_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(model, data_module)\ntest_results = trainer.test(model, datamodule=data_module)\nprint(\"Printing test results...\")\nprint(test_results)\nmodel_mses.append(test_results[0][\"test_mse\"])\n\n# linear model\nlinear_model = get_linear_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(linear_model, data_module)\ntest_results = trainer.test(linear_model, datamodule=data_module)\nprint(\"Printing linear model test results\")\nprint(test_results)\nlinear_model_test_mses.append(test_results[0][\"test_mse\"])\n</code></pre> <p>Train models on data generated using the binary relations between TFs (method 4)</p> <pre><code>tf_relationships_dict_boolean_logic = {\n    0: [And(3, 4, 8), Or(3, 7), Or(1, 1)],\n    1: [And(5, Or(7, 8))],\n    2: [],\n    3: [Or(7, 9), And(6, 7)],\n    4: [And(1, 2)],\n    5: [Or(0, 1, 2, 8, 9)],\n    6: [And(0, Or(1, 2))],\n    7: [Or(2, And(5, 6, 9))],\n    8: [],\n    9: [And(6, And(3, Or(0, 9)))],\n}\n\ndata_module = get_data_module(\n    3.0, \n    adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic, \n    tf_relationships_dict=tf_relationships_dict_boolean_logic\n)\n\n# nonlinear model\nmodel = get_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(model, data_module)\ntest_results = trainer.test(model, datamodule=data_module)\nprint(\"Printing test results...\")\nprint(test_results)\nmodel_mses.append(test_results[0][\"test_mse\"])\n\n# linear model\nlinear_model = get_linear_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(linear_model, data_module)\ntest_results = trainer.test(linear_model, datamodule=data_module)\nprint(\"Printing linear model test results\")\nprint(test_results)\nlinear_model_test_mses.append(test_results[0][\"test_mse\"])\n</code></pre> <p>Now we can plot the results of our experiment. TODO add explantion for plot here? Probably not the right place to put it (I feel like that belongs in the presentation or something, because this notebook could be modified and the explanation wouldn\u2019t make sense)</p> <pre><code>data_gen_methods = [\"No Mean Adjustment\", \"Dependent Mean Adjustment\", \"TF Dependent Mean Adjustment\", \"TF Dependent Mean Adjust with Boolean Logic\"]\nplt.figure(figsize=(10, 6))\nplt.scatter(data_gen_methods, model_mses, color='blue')\nplt.scatter(data_gen_methods, linear_model_test_mses, color='orange')\nplt.title('Model MSE Comparison (bound mean = 3.0)')\nplt.xlabel('Model')\nplt.ylabel('MSE')\nplt.grid(True)\nplt.xticks(rotation=45, ha=\"right\")\nplt.legend(['Complex (Customizable) Model', 'Linear Model'])\nplt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels\nplt.show()\n</code></pre>"},{"location":"tutorials/visualizing_and_testing_data_generation_methods/#training-models-on-data-generated-from-the-4-different-methods","title":"Training models on data generated from the 4 different methods","text":"<p>In the next experiment, we will be training the exact same model on data generated from each of these 4 methods. We will also train a simple linear model on all four methods to use as a baseline to compare to. Other than the method used to generate the data, everything else will be held the same.</p>"}]}