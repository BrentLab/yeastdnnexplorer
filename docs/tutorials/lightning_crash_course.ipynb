{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning Crash Course\n",
    "This project uses the PyTorch Lightning Library to define and train the machine learning models. PyTorch Lightning is built on top of pytorch, and it abstracts away some of the setup and biolerplate for models (such as writing out training loops). In this notebook, we provide a brief introduction to how to use the models and dataModules we've defined to train models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "\n",
    "from yeastdnnexplorer.data_loaders.synthetic_data_loader import SyntheticDataLoader\n",
    "from yeastdnnexplorer.data_loaders.real_data_loader import RealDataLoader\n",
    "from yeastdnnexplorer.ml_models.simple_model import SimpleModel\n",
    "from yeastdnnexplorer.ml_models.customizable_model import CustomizableModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pytorch Lightning, the data is kept completely separate from the models. This allows for you to easy train a model using different datasets or train different models on the same dataset. `DataModules` encapsulate all the logic of loading in a specific dataset and splitting into training, testing, and validation sets. In this project, we have two data loaders defined: `SyntheticDataLoader` for the in silico data (which takes in many parameters that allow you to specify how the data is generated) and `RealDataLoader` which contains all of the logic for loading in the real experiment data and putting it into a form that the models expect.\n",
    "\n",
    "Once you decide what model you want to train and what dataModule you want to use, you can bundle these with a `Trainer` object to train the model on the dataset.\n",
    "\n",
    "If you'd like to learn more about the models and dataModules we've defined, there is extensive documentation in each of the files that explains each method's purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an instance of our simple linear baseline model\n",
    "model = SimpleModel(\n",
    "    input_dim=10,\n",
    "    output_dim=10,\n",
    "    lr=1e-2,\n",
    ")\n",
    "\n",
    "# define an instance of the synthetic data loader\n",
    "# see the constructor for the full list of params and their explanations\n",
    "data_module = SyntheticDataLoader(\n",
    "    batch_size=32,\n",
    "    num_genes=3000,\n",
    "    signal=[0.5] * 5,\n",
    "    n_sample=[1, 1, 2, 2, 4],\n",
    "    val_size=0.1,\n",
    "    test_size=0.1,\n",
    "    signal_mean=3.0,\n",
    ")\n",
    "\n",
    "# define a trainer instance\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True,\n",
    "    accelerator=\"cpu\", # change to \"gpu\" if you have access to one\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# test the model (recall that data_module specifies the train / test split, we don't need to do it explicitly here)\n",
    "test_results = trainer.test(model, data_module)\n",
    "print(test_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very easy to train the same model on a different dataset, for example if we want to use real world data we can just swap to the data module that we've defined for the real world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to redefine a new instance with the same params unless we want it to pick up where it left off\n",
    "new_model = SimpleModel(\n",
    "    input_dim=30,  # note that the input and output dims are equal to the num TFs in the dataset\n",
    "    output_dim=30,\n",
    "    lr=1e-2,\n",
    ")\n",
    "\n",
    "real_data_module = RealDataLoader(\n",
    "    batch_size=32,\n",
    "    val_size=0.1,\n",
    "    test_size=0.1,\n",
    "    data_dir_path=\"../../data/init_analysis_data_20240409/\", # note that this is relative to where real_data_loader.py is\n",
    "    perturbation_dataset_title=\"hu_reimann_tfko\",\n",
    ")\n",
    "\n",
    "# we also have to define a new trainer instance, not really sure why but it seems to be necessary\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True,\n",
    "    accelerator=\"cpu\", # change to \"gpu\" if you have access to one\n",
    ")\n",
    "\n",
    "trainer.fit(new_model, real_data_module)\n",
    "test_results = trainer.test(new_model, real_data_module)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to do the same thing with our more complex and customizable `CustomizableModel` (which allows you to pass in many params like the number of hidden layers, dropout rate, choice of optimizer, etc) the code would look identical to above except that we would be initializing a `CustomizableModel` instead of a `SimpleModel`. See the documentation in `customizable_model.py` for more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointing & Logging\n",
    "PyTorch lightning gives us the power to define checkpoints and loggers that will be used during training. Checkpoints will save checkpoints of your model during training. In the following code, we define a checkpoint that saves the model's state when it produced the lowest validation mean squared error on the validation set during training. We also define another checkpoint to periodically save a checkpoint of the model after every 2 training epochs. These checkpoints are powerful because they can be reloaded later. You can continue training a model after loading its checkpoint or you can test the model checkpoint on new data.\n",
    "\n",
    "Loggers are responsible for saving metrics about the model as it is training for us to look at later. We define several loggers to track this data. See the comments above the Tensorboard logger to see how to use Tensorboard to visualize the metrics as the model trains\n",
    "\n",
    "To use checkpoints and loggers, we have to pass them into the Trainer object that we use to train the model with a dataModule. \n",
    "\n",
    "There are many more types of checkpoints and loggers you can create and use, PyTorch Lightning's documentation is very helpful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be used to save the model checkpoint that performs the best on the validation set\n",
    "best_model_checkpoint = ModelCheckpoint(\n",
    "    monitor=\"val_mse\", # we can depend on any metric we want\n",
    "    mode=\"min\",\n",
    "    filename=\"best-model-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1, # we can save more than just the top model if we want\n",
    ")\n",
    "\n",
    "# Callback to save checkpoints every 2 epochs, regardless of model performance\n",
    "periodic_checkpoint = ModelCheckpoint(\n",
    "    filename=\"periodic-{epoch:02d}\",\n",
    "    every_n_epochs=2,\n",
    "    save_top_k=-1,  # Setting -1 saves all checkpoints  \n",
    ")\n",
    "\n",
    "# csv logger is a very basic logger that will create a csv file with our metrics as we train\n",
    "csv_logger = CSVLogger(\"logs/csv_logs\")  # we define the directory we want the logs to be saved in\n",
    "\n",
    "# tensorboard logger is a more advanced logger that will create a directory with a bunch of files that can be visualized with tensorboard\n",
    "# tensorboard is a library that can be ran via the command line, and will create a local server that can be accessed via a web browser\n",
    "# that displays the training metrics in a more interactive way (on a dashboard)\n",
    "# you can run tensorboard by running the command `tensorboard --logdir=path/to/log/dir` in the terminal\n",
    "tb_logger = TensorBoardLogger(\"logs/tensorboard_logs\", name=\"test-run-2\")\n",
    "\n",
    "# If we wanted to use these checkpoints and loggers, we would pass them to the trainer like so:\n",
    "trainer_with_checkpoints_and_loggers = Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True,\n",
    "    accelerator=\"cpu\",\n",
    "    callbacks=[best_model_checkpoint, periodic_checkpoint],\n",
    "    logger=[csv_logger, tb_logger],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in and using a Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model from a checkpoint\n",
    "# We can load a model from a checkpoint like so:\n",
    "path_to_checkpoint = \"example/path/not/real.ckpt\"\n",
    "\n",
    "# note that we need to use the same model class that was used to save the checkpoint\n",
    "model = SimpleModel.load_from_checkpoint(path_to_checkpoint)\n",
    "\n",
    "# we can load the model and continue training from where it left off\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# we could also load the model and test it\n",
    "test_results = trainer.test(model, data_module)\n",
    "\n",
    "# we could also load the model and make predictions\n",
    "predictions = model(data_module.test_dataloader())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
