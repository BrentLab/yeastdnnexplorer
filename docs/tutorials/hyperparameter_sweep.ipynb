{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces how to perform a hyperparameter sweep to find the best hyperparameters for our model using the Optuna library. Feel free to modify the objective function if you would like to test other hyperparameters or values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports \n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "\n",
    "from pytorch_lightning import Trainer, LightningModule, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "from torchsummary import summary\n",
    "\n",
    "from yeastdnnexplorer.data_loaders.synthetic_data_loader import SyntheticDataLoader\n",
    "from yeastdnnexplorer.ml_models.simple_model import SimpleModel\n",
    "from yeastdnnexplorer.ml_models.customizable_model import CustomizableModel\n",
    "\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# set random seed for reproducability\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define loggers and checkpoints for our model. Checkpoints tell pytorch when to save instances of the model (that can be loaded and inspected later) and loggers tell pytorch how to format the metrics that the model logs during its training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint to save the best version of model (during the entire training process) based on the metric passed into \"monitor\"\n",
    "best_model_checkpoint = ModelCheckpoint(\n",
    "    monitor=\"val_mse\",  # You can modify this to save the best model based on any other metric that the model you're testing tracks and reports\n",
    "    mode=\"min\",\n",
    "    filename=\"best-model-{epoch:02d}-{val_loss:.2f}.ckpt\",\n",
    "    save_top_k=1,  # Can modify this to save the top k models\n",
    ")\n",
    "\n",
    "# Callback to save checkpoints every 2 epochs, regardless of performance\n",
    "periodic_checkpoint = ModelCheckpoint(\n",
    "    filename=\"periodic-{epoch:02d}.ckpt\",\n",
    "    every_n_epochs=2,\n",
    "    save_top_k=-1,  # Setting -1 saves all checkpoints\n",
    ")\n",
    "\n",
    "# define loggers for the model\n",
    "tb_logger = TensorBoardLogger(\"logs/tensorboard_logs\")\n",
    "csv_logger = CSVLogger(\"logs/csv_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform our hyperparameter sweep using the Optuna library. To do this, we need to define an objective function that returns a scalar value. This scalar value will be the value that our sweep is attempting to minimize. We train one instance of our model inside each call to the objective function (each model on each iteration will use a different selection of hyperparameters). In our objective function, we return the validation mse associated with the instance of the model. This is because we would like to find the combination of hyperparameters that leads to the lowest validation mse. We use validation mse instead of test mse since we do not want to risk fitting to the test data at all while tuning hyperparameters.\n",
    "\n",
    "If you'd like to try different hyperparameters, you just need to modify the list of possible values corresponding to the hyperparameter in question.\n",
    "\n",
    "If you'd like to run the hyperparamter sweep on real data instead of synthetic data, simply swap out the synthetic data loader for the real data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on each call to the objective function, it will choose a hyperparameter value from each of the suggest_categorical arrays and pass them into the model\n",
    "    # this allows us to test many different hyperparameter configurations during our sweep\n",
    "\n",
    "def objective(trial):\n",
    "    # model hyperparameters\n",
    "    lr = trial.suggest_categorical(\"lr\", [0.01])\n",
    "    hidden_layer_num = trial.suggest_categorical(\"hidden_layer_num\", [1, 2, 3, 5])\n",
    "    activation = trial.suggest_categorical(\n",
    "        \"activation\", [\"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\"]\n",
    "    )\n",
    "    optimizer = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\"])\n",
    "    L2_regularization_term = trial.suggest_categorical(\n",
    "        \"L2_regularization_term\", [0.0, 0.1]\n",
    "    )\n",
    "    dropout_rate = trial.suggest_categorical(\n",
    "        \"dropout_rate\", [0.0, 0.5]\n",
    "    )\n",
    "\n",
    "    # data module hyperparameters\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32])\n",
    "\n",
    "    # training hyperparameters\n",
    "    max_epochs = trial.suggest_categorical(\n",
    "        \"max_epochs\", [1]\n",
    "    ) # default is 10\n",
    "\n",
    "    # defining what to pass in for the hidden layer sizes list based on the number of hidden layers\n",
    "    hidden_layer_sizes_configurations = {\n",
    "        1: [[64], [256]],\n",
    "        2: [[64, 32], [256, 64]],\n",
    "        3: [[256, 128, 32], [512, 256, 64]],\n",
    "        5: [[512, 256, 128, 64, 32]],\n",
    "    }\n",
    "    hidden_layer_sizes = trial.suggest_categorical(\n",
    "        f\"hidden_layer_sizes_{hidden_layer_num}_layers\",\n",
    "        hidden_layer_sizes_configurations[hidden_layer_num],\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"About to create model with the following hyperparameters:\")\n",
    "    print(f\"lr: {lr}\")\n",
    "    print(f\"hidden_layer_num: {hidden_layer_num}\")\n",
    "    print(f\"hidden_layer_sizes: {hidden_layer_sizes}\")\n",
    "    print(f\"activation: {activation}\")\n",
    "    print(f\"optimizer: {optimizer}\")\n",
    "    print(f\"L2_regularization_term: {L2_regularization_term}\")\n",
    "    print(f\"dropout_rate: {dropout_rate}\")\n",
    "    print(f\"batch_size: {batch_size}\")\n",
    "    print(f\"max_epochs: {max_epochs}\")\n",
    "    print(\"\")\n",
    "\n",
    "    # create data module\n",
    "    data_module = SyntheticDataLoader(\n",
    "        batch_size=batch_size,\n",
    "        num_genes=4000,\n",
    "        signal_mean=3.0,\n",
    "        signal=[0.5] * 10,\n",
    "        n_sample=[1, 2, 2, 4, 4],\n",
    "        val_size=0.1,\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "        max_mean_adjustment=3.0,\n",
    "    )\n",
    "\n",
    "    num_tfs = sum(data_module.n_sample)  # sum of all n_sample is the number of TFs\n",
    "\n",
    "    # create model\n",
    "    model = CustomizableModel(\n",
    "        input_dim=num_tfs,\n",
    "        output_dim=num_tfs,\n",
    "        lr=lr,\n",
    "        hidden_layer_num=hidden_layer_num,\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        optimizer=optimizer,\n",
    "        L2_regularization_term=L2_regularization_term,\n",
    "        dropout_rate=dropout_rate,\n",
    "    )\n",
    "\n",
    "    # create trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        deterministic=True,\n",
    "        accelerator=\"cpu\",\n",
    "        # callbacks and loggers are commented out for now since running a large sweep would generate an unnecessarily huge amount of checkpoints and logs\n",
    "        # callbacks=[best_model_checkpoint, periodic_checkpoint],\n",
    "        # logger=[tb_logger, csv_logger],\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    trainer.fit(model, data_module)\n",
    "\n",
    "    # get best validation loss from the model\n",
    "    return trainer.callback_metrics[\"val_mse\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define an optuna study, which represents our hyperparameter sweep. It will run the objective function n_trials times and choose the model that gave the best val_mse across all of those trials with different hyperparameters. Note that this will create a very large amount of output as it will show training stats for every model. This is why we print out the best params and loss in a separate cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_NAME = \"CustomizableModelHyperparameterSweep3\"\n",
    "NUM_TRIALS = 5 # you will need a lot more than 5 trials if you have many possible combinations of hyperparams\n",
    "\n",
    "# Perform hyperparameter optimization using Optuna\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\", # we want to minimize the val_mse\n",
    "    study_name=STUDY_NAME,\n",
    "    # storage=\"sqlite:///db.sqlite3\", # you can save the study results in a database if you'd like, this is needed if you want to try and use the optuna dashboard library to dispaly results\n",
    ")\n",
    "study.optimize(objective, n_trials=NUM_TRIALS)\n",
    "\n",
    "# Get the best hyperparameters and their corresponding values\n",
    "best_params = study.best_params\n",
    "best_loss = study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the best hyperparameters and the val_mse assocaited with the model with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESULTS\" + (\"=\" * 70))\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Best loss: {best_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Now you could take what you found to be the best hyperparameters and train a model with them for many more epochs. The [Optuna Documentation](https://optuna.readthedocs.io/en/stable/) will be a helpful resource if you'd like to add more to this notebook or the hyperparam sweep functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
